{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data download preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 19:43:59,828 INFO [2024-09-28T00:00:00] **Welcome to the New Climate Data Store (CDS)!** This new system is in its early days of full operations and still undergoing enhancements and fine tuning. Some disruptions are to be expected. Your \n",
      "[feedback](https://jira.ecmwf.int/plugins/servlet/desk/portal/1/create/202) is key to improve the user experience on the new CDS for the benefit of everyone. Thank you.\n",
      "2024-11-17 19:43:59,832 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2024-11-17 19:43:59,833 INFO [2024-09-16T00:00:00] Remember that you need to have an ECMWF account to use the new CDS. **Your old CDS credentials will not work in new CDS!**\n",
      "2024-11-17 19:43:59,834 WARNING [2024-06-16T00:00:00] CDS API syntax is changed and some keys or parameter names may have also changed. To avoid requests failing, please use the \"Show API request code\" tool on the dataset Download Form to check you are using the correct syntax for your API request.\n",
      "2024-11-17 19:43:59,836 - INFO - Retrieving data for 2020-11\n",
      "2024-11-17 19:44:01,673 WARNING [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-17 19:44:01,673 - WARNING - [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-17 19:44:01,675 INFO Request ID is 207ad4bc-9721-4e16-b963-f07db3bd4aa4\n",
      "2024-11-17 19:44:01,675 - INFO - Request ID is 207ad4bc-9721-4e16-b963-f07db3bd4aa4\n",
      "2024-11-17 19:44:02,589 INFO status has been updated to accepted\n",
      "2024-11-17 19:44:02,589 - INFO - status has been updated to accepted\n",
      "2024-11-17 19:44:04,312 INFO status has been updated to running\n",
      "2024-11-17 19:44:04,312 - INFO - status has been updated to running\n",
      "2024-11-17 19:44:58,101 INFO status has been updated to successful\n",
      "2024-11-17 19:44:58,101 - INFO - status has been updated to successful\n",
      "2024-11-17 19:44:59,489 - INFO - Downloading https://object-store.os-api.cci2.ecmwf.int:443/cci2-prod-cache/94596468c920d7733b6fc1a69e5ded11.nc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918f3f0a159144b2a33365f8a95097e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "94596468c920d7733b6fc1a69e5ded11.nc:   0%|          | 0.00/48.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 19:45:27,284 - INFO - Successfully downloaded era5_data\\era5_2020_11.nc\n",
      "2024-11-17 19:45:32,294 - INFO - Retrieving data for 2020-12\n",
      "2024-11-17 19:45:34,386 WARNING [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-17 19:45:34,386 - WARNING - [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-17 19:45:34,389 INFO Request ID is f073a9bf-6396-4e7c-a71d-63b2bac29bcc\n",
      "2024-11-17 19:45:34,389 - INFO - Request ID is f073a9bf-6396-4e7c-a71d-63b2bac29bcc\n",
      "2024-11-17 19:45:34,817 INFO status has been updated to accepted\n",
      "2024-11-17 19:45:34,817 - INFO - status has been updated to accepted\n",
      "2024-11-17 19:45:38,364 INFO status has been updated to running\n",
      "2024-11-17 19:45:38,364 - INFO - status has been updated to running\n",
      "2024-11-17 19:46:27,992 INFO status has been updated to successful\n",
      "2024-11-17 19:46:27,992 - INFO - status has been updated to successful\n",
      "2024-11-17 19:46:30,021 - INFO - Downloading https://object-store.os-api.cci2.ecmwf.int:443/cci2-prod-cache/17191cfae8a817905dd3846f2ad73ace.nc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218d9a4f330e4317b137be6e9ad316ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "17191cfae8a817905dd3846f2ad73ace.nc:   0%|          | 0.00/51.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 19:46:48,966 - INFO - Successfully downloaded era5_data\\era5_2020_12.nc\n",
      "2024-11-17 19:46:53,971 - INFO - Retrieving data for 2020-01\n",
      "2024-11-17 19:46:55,335 WARNING [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-17 19:46:55,335 - WARNING - [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-17 19:46:55,337 INFO Request ID is 31a9c734-f17a-407c-b993-aaf87ef33984\n",
      "2024-11-17 19:46:55,337 - INFO - Request ID is 31a9c734-f17a-407c-b993-aaf87ef33984\n",
      "2024-11-17 19:46:55,804 INFO status has been updated to accepted\n",
      "2024-11-17 19:46:55,804 - INFO - status has been updated to accepted\n",
      "2024-11-17 19:46:59,404 INFO status has been updated to running\n",
      "2024-11-17 19:46:59,404 - INFO - status has been updated to running\n",
      "2024-11-17 19:47:06,101 INFO status has been updated to accepted\n",
      "2024-11-17 19:47:06,101 - INFO - status has been updated to accepted\n",
      "2024-11-17 19:47:11,584 INFO status has been updated to running\n",
      "2024-11-17 19:47:11,584 - INFO - status has been updated to running\n",
      "2024-11-17 19:48:15,531 INFO status has been updated to successful\n",
      "2024-11-17 19:48:15,531 - INFO - status has been updated to successful\n",
      "2024-11-17 19:48:19,888 - INFO - Downloading https://object-store.os-api.cci2.ecmwf.int:443/cci2-prod-cache/b5713c0fc2e93e5eaa2f73fe5d72d751.nc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9e71620c6046e7a9868c0a4ace2223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "b5713c0fc2e93e5eaa2f73fe5d72d751.nc:   0%|          | 0.00/50.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 19:48:30,896 - INFO - Successfully downloaded era5_data\\era5_2020_01.nc\n",
      "2024-11-17 19:48:35,905 - INFO - Retrieving data for 2020-02\n",
      "2024-11-17 19:48:37,539 WARNING [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-17 19:48:37,539 - WARNING - [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-17 19:48:37,542 INFO Request ID is 0b94c20e-24f8-4509-afc5-49609dfb813f\n",
      "2024-11-17 19:48:37,542 - INFO - Request ID is 0b94c20e-24f8-4509-afc5-49609dfb813f\n",
      "2024-11-17 19:48:37,970 INFO status has been updated to accepted\n",
      "2024-11-17 19:48:37,970 - INFO - status has been updated to accepted\n",
      "2024-11-17 19:48:41,399 INFO status has been updated to running\n",
      "2024-11-17 19:48:41,399 - INFO - status has been updated to running\n",
      "2024-11-17 19:49:31,694 INFO status has been updated to successful\n",
      "2024-11-17 19:49:31,694 - INFO - status has been updated to successful\n",
      "2024-11-17 19:49:33,235 - INFO - Downloading https://object-store.os-api.cci2.ecmwf.int:443/cci2-prod-cache/7a47754be08ac75edc9bf9dec591a98b.nc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75d31e553d24c6abc79b71ca9a97bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "7a47754be08ac75edc9bf9dec591a98b.nc:   0%|          | 0.00/47.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 19:50:03,310 - INFO - Successfully downloaded era5_data\\era5_2020_02.nc\n",
      "2024-11-17 19:50:08,319 - INFO - Retrieving data for 2020-03\n",
      "2024-11-17 19:50:11,170 WARNING [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-17 19:50:11,170 - WARNING - [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-17 19:50:11,172 INFO Request ID is 28ef174a-636a-4b03-909a-1e7c5086a92a\n",
      "2024-11-17 19:50:11,172 - INFO - Request ID is 28ef174a-636a-4b03-909a-1e7c5086a92a\n",
      "2024-11-17 19:50:11,870 INFO status has been updated to accepted\n",
      "2024-11-17 19:50:11,870 - INFO - status has been updated to accepted\n",
      "2024-11-17 19:50:15,816 INFO status has been updated to running\n",
      "2024-11-17 19:50:15,816 - INFO - status has been updated to running\n",
      "2024-11-17 19:50:22,869 INFO status has been updated to accepted\n",
      "2024-11-17 19:50:22,869 - INFO - status has been updated to accepted\n",
      "2024-11-17 19:50:28,748 INFO status has been updated to running\n",
      "2024-11-17 19:50:28,748 - INFO - status has been updated to running\n",
      "2024-11-17 19:51:06,397 INFO status has been updated to successful\n",
      "2024-11-17 19:51:06,397 - INFO - status has been updated to successful\n",
      "2024-11-17 19:51:10,189 - INFO - Downloading https://object-store.os-api.cci2.ecmwf.int:443/cci2-prod-cache/493d1a84364ee0b4998c1b8e44a6261.nc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09451589a14e4eef8cb090fed26a642f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "493d1a84364ee0b4998c1b8e44a6261.nc:   0%|          | 0.00/49.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 19:51:46,539 - INFO - Successfully downloaded era5_data\\era5_2020_03.nc\n",
      "2024-11-17 19:51:51,548 - INFO - Retrieving data for 2021-11\n",
      "2024-11-17 19:51:53,365 WARNING [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-17 19:51:53,365 - WARNING - [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-17 19:51:53,367 INFO Request ID is 8a711334-6d14-42f2-a3ce-98cfcd8aa350\n",
      "2024-11-17 19:51:53,367 - INFO - Request ID is 8a711334-6d14-42f2-a3ce-98cfcd8aa350\n",
      "2024-11-17 19:51:54,041 INFO status has been updated to accepted\n",
      "2024-11-17 19:51:54,041 - INFO - status has been updated to accepted\n",
      "2024-11-17 19:52:07,086 INFO status has been updated to running\n",
      "2024-11-17 19:52:07,086 - INFO - status has been updated to running\n",
      "2024-11-17 19:52:50,858 INFO status has been updated to successful\n",
      "2024-11-17 19:52:50,858 - INFO - status has been updated to successful\n",
      "2024-11-17 19:52:51,913 - INFO - Downloading https://object-store.os-api.cci2.ecmwf.int:443/cci2-prod-cache/f363bef6e70757b46cc5f8eb363b3dc1.nc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267eb0664f764768947e09a6f6b83ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "f363bef6e70757b46cc5f8eb363b3dc1.nc:   0%|          | 0.00/49.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 19:53:15,837 - INFO - Successfully downloaded era5_data\\era5_2021_11.nc\n",
      "2024-11-17 19:53:20,853 - INFO - Retrieving data for 2021-12\n",
      "2024-11-17 19:53:22,355 WARNING [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-17 19:53:22,355 - WARNING - [2024-10-10T00:00:00] The final validated ERA5 differs from ERA5T from July 2024 until further notice - please refer to our\n",
      "[Forum announcement](https://forum.ecmwf.int/t/final-validated-era5-product-to-differ-from-era5t-in-july-2024/6685)\n",
      "for details and watch it for further updates on this.\n",
      "2024-11-17 19:53:22,357 INFO Request ID is adf569ae-7221-4d99-a9ce-a8bc55ea09c8\n",
      "2024-11-17 19:53:22,357 - INFO - Request ID is adf569ae-7221-4d99-a9ce-a8bc55ea09c8\n",
      "2024-11-17 19:53:22,787 INFO status has been updated to accepted\n",
      "2024-11-17 19:53:22,787 - INFO - status has been updated to accepted\n",
      "2024-11-17 19:53:26,447 INFO status has been updated to running\n",
      "2024-11-17 19:53:26,447 - INFO - status has been updated to running\n",
      "2024-11-17 19:54:17,789 INFO status has been updated to successful\n",
      "2024-11-17 19:54:17,789 - INFO - status has been updated to successful\n",
      "2024-11-17 19:54:18,716 - INFO - Downloading https://object-store.os-api.cci2.ecmwf.int:443/cci2-prod-cache/d3b81914980bbfd43caa545ca89d866.nc\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b302820f314b6e93ec93fd63605407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "d3b81914980bbfd43caa545ca89d866.nc:   0%|          | 0.00/51.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 19:54:36,617 - INFO - Successfully downloaded era5_data\\era5_2021_12.nc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data retrieval completed!\n"
     ]
    }
   ],
   "source": [
    "import cdsapi\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "class ERA5Retriever:\n",
    "    def __init__(self, output_dir='era5_data'):\n",
    "        \"\"\"初始化ERA5数据获取器\"\"\"\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.client = cdsapi.Client()\n",
    "        self.setup_logging()\n",
    "\n",
    "    def setup_logging(self):\n",
    "        \"\"\"设置日志记录\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(self.output_dir / 'era5_retrieval.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger('ERA5Retriever')\n",
    "\n",
    "    def retrieve_data(self, year, month, output_file=None):\n",
    "        \"\"\"获取ERA5数据\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        year : int\n",
    "            年份\n",
    "        month : int\n",
    "            月份\n",
    "        output_file : str, optional\n",
    "            输出文件名\n",
    "        \"\"\"\n",
    "        if output_file is None:\n",
    "            output_file = self.output_dir / f'era5_{year}_{month:02d}.nc'\n",
    "\n",
    "        # 如果文件已存在，跳过下载\n",
    "        if output_file.exists():\n",
    "            self.logger.info(f\"File {output_file} already exists, skipping...\")\n",
    "            return output_file\n",
    "\n",
    "        try:\n",
    "            # 构建请求参数\n",
    "            request = {\n",
    "                \"format\": \"netcdf\",\n",
    "                \"product_type\": \"reanalysis\",\n",
    "                \"variable\": [\n",
    "                    \"2m_temperature\",                    # 温度\n",
    "                    \"total_precipitation\",               # 降水\n",
    "                    \"mean_sea_level_pressure\",          # 海平面气压\n",
    "                    \"10m_u_component_of_wind\",          # 风场\n",
    "                    \"10m_v_component_of_wind\",\n",
    "                    \"geopotential_at_500hpa\",           # 500hPa位势高度\n",
    "                    \"relative_humidity_at_850hpa\",      # 850hPa相对湿度\n",
    "                ],\n",
    "                \"year\": str(year),\n",
    "                \"month\": f\"{month:02d}\",\n",
    "                \"day\": [f\"{day:02d}\" for day in range(1, 32)],\n",
    "                \"time\": [f\"{hour:02d}:00\" for hour in range(0, 24, 6)],  # 6小时间隔\n",
    "                \"area\": [70, -20, 30, 60],  # 欧洲区域 [North, West, South, East]\n",
    "            }\n",
    "\n",
    "            # 获取数据\n",
    "            self.logger.info(f\"Retrieving data for {year}-{month:02d}\")\n",
    "            self.client.retrieve(\n",
    "                'reanalysis-era5-single-levels',\n",
    "                request,\n",
    "                output_file\n",
    "            )\n",
    "            self.logger.info(f\"Successfully downloaded {output_file}\")\n",
    "            \n",
    "            # 添加间隔，避免请求过于频繁\n",
    "            time.sleep(5)\n",
    "            \n",
    "            return output_file\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error retrieving data for {year}-{month:02d}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数示例用法\"\"\"\n",
    "    retriever = ERA5Retriever()\n",
    "\n",
    "    # 定义时间范围（按照标杆论文，我们需要1959-2021的数据）\n",
    "    # 但建议先下载小部分数据测试\n",
    "    years = [2020, 2021]  # 测试用年份\n",
    "    months = [11, 12, 1, 2, 3]  # 冬季月份\n",
    "\n",
    "    # 获取数据\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            if month in [1, 2, 3] and year == years[-1]:\n",
    "                continue  # 跳过最后一年的1-3月\n",
    "            retriever.retrieve_data(year, month)\n",
    "\n",
    "    print(\"Data retrieval completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据...\n",
      "Dataset dimensions: ['valid_time', 'latitude', 'longitude']\n",
      "Dataset variables: ['t2m', 'tp', 'msl', 'u10', 'v10']\n",
      "\n",
      "计算基本统计信息：\n",
      "              mean          std           min            max\n",
      "t2m     276.098755    10.294481    235.358200     306.281921\n",
      "tp        0.000095     0.000280      0.000000       0.009905\n",
      "msl  101433.968750  1176.391968  95805.125000  104607.125000\n",
      "u10       0.678640     4.715758    -22.936874      28.554581\n",
      "v10       0.627906     4.453432    -24.695114      23.146225\n",
      "\n",
      "绘制空间分布图...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda3\\envs\\environment-config\\lib\\site-packages\\cartopy\\io\\__init__.py:241: DownloadWarning: Downloading: https://naturalearth.s3.amazonaws.com/50m_physical/ne_50m_coastline.zip\n",
      "  warnings.warn(f'Downloading: {url}', DownloadWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存 t2m 的空间分布图\n",
      "已保存 tp 的空间分布图\n",
      "已保存 msl 的空间分布图\n",
      "已保存 u10 的空间分布图\n",
      "已保存 v10 的空间分布图\n",
      "\n",
      "绘制时间序列...\n",
      "已保存 t2m 的时间序列图\n",
      "已保存 tp 的时间序列图\n",
      "已保存 msl 的时间序列图\n",
      "已保存 u10 的时间序列图\n",
      "已保存 v10 的时间序列图\n",
      "\n",
      "分析日变化...\n",
      "分析 t2m 的日变化时出错: `group` must be an xarray.DataArray or the name of an xarray variable or dimension. Received Index([ 0,  6, 12, 18,  0,  6, 12, 18,  0,  6,\n",
      "       ...\n",
      "       12, 18,  0,  6, 12, 18,  0,  6, 12, 18],\n",
      "      dtype='int32', length=124) instead.\n",
      "分析 tp 的日变化时出错: `group` must be an xarray.DataArray or the name of an xarray variable or dimension. Received Index([ 0,  6, 12, 18,  0,  6, 12, 18,  0,  6,\n",
      "       ...\n",
      "       12, 18,  0,  6, 12, 18,  0,  6, 12, 18],\n",
      "      dtype='int32', length=124) instead.\n",
      "分析 msl 的日变化时出错: `group` must be an xarray.DataArray or the name of an xarray variable or dimension. Received Index([ 0,  6, 12, 18,  0,  6, 12, 18,  0,  6,\n",
      "       ...\n",
      "       12, 18,  0,  6, 12, 18,  0,  6, 12, 18],\n",
      "      dtype='int32', length=124) instead.\n",
      "分析 u10 的日变化时出错: `group` must be an xarray.DataArray or the name of an xarray variable or dimension. Received Index([ 0,  6, 12, 18,  0,  6, 12, 18,  0,  6,\n",
      "       ...\n",
      "       12, 18,  0,  6, 12, 18,  0,  6, 12, 18],\n",
      "      dtype='int32', length=124) instead.\n",
      "分析 v10 的日变化时出错: `group` must be an xarray.DataArray or the name of an xarray variable or dimension. Received Index([ 0,  6, 12, 18,  0,  6, 12, 18,  0,  6,\n",
      "       ...\n",
      "       12, 18,  0,  6, 12, 18,  0,  6, 12, 18],\n",
      "      dtype='int32', length=124) instead.\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "\n",
    "class ERA5Analyzer:\n",
    "    def __init__(self, data_dir='era5_data'):\n",
    "        \"\"\"初始化分析器\"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        \n",
    "    def load_data(self, filename):\n",
    "        \"\"\"加载并检查ERA5数据\"\"\"\n",
    "        file_path = self.data_dir / filename\n",
    "        ds = xr.open_dataset(file_path)\n",
    "        \n",
    "        # 打印数据集信息\n",
    "        print(\"Dataset dimensions:\", list(ds.dims))\n",
    "        print(\"Dataset variables:\", list(ds.data_vars))\n",
    "        return ds\n",
    "    \n",
    "    def basic_statistics(self, ds):\n",
    "        \"\"\"计算基本统计信息\"\"\"\n",
    "        stats = {}\n",
    "        for var in ds.data_vars:\n",
    "            stats[var] = {\n",
    "                'mean': float(ds[var].mean()),\n",
    "                'std': float(ds[var].std()),\n",
    "                'min': float(ds[var].min()),\n",
    "                'max': float(ds[var].max())\n",
    "            }\n",
    "        return pd.DataFrame(stats).T\n",
    "    \n",
    "    def plot_spatial_mean(self, ds, variable, title=None):\n",
    "        \"\"\"绘制空间平均分布\"\"\"\n",
    "        fig = plt.figure(figsize=(15, 10))\n",
    "        ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "        \n",
    "        # 计算时间平均（使用valid_time而不是time）\n",
    "        mean_data = ds[variable].mean(dim='valid_time')\n",
    "        \n",
    "        # 绘制地图\n",
    "        mean_data.plot(\n",
    "            ax=ax,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            cmap='RdBu_r',\n",
    "            robust=True\n",
    "        )\n",
    "        \n",
    "        ax.coastlines()\n",
    "        ax.gridlines()\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "            \n",
    "        return fig\n",
    "    \n",
    "    def plot_time_series(self, ds, variable, lat=None, lon=None):\n",
    "        \"\"\"绘制时间序列\"\"\"\n",
    "        if lat is not None and lon is not None:\n",
    "            # 提取特定位置的数据\n",
    "            data = ds[variable].sel(latitude=lat, longitude=lon, method='nearest')\n",
    "        else:\n",
    "            # 计算空间平均\n",
    "            data = ds[variable].mean(dim=['latitude', 'longitude'])\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(15, 5))\n",
    "        data.plot(x='valid_time', ax=ax)  # 使用valid_time作为x轴\n",
    "        plt.title(f'{variable} Time Series')\n",
    "        return fig\n",
    "    \n",
    "    def calculate_anomalies(self, ds, climatology_period=None):\n",
    "        \"\"\"计算异常值\"\"\"\n",
    "        # 确保使用valid_time\n",
    "        time_coord = 'valid_time'\n",
    "        \n",
    "        if climatology_period is None:\n",
    "            climatology = ds.groupby(f'{time_coord}.dayofyear').mean(time_coord)\n",
    "            anomalies = ds.groupby(f'{time_coord}.dayofyear') - climatology\n",
    "        else:\n",
    "            start_year, end_year = climatology_period\n",
    "            mask = (ds[time_coord].dt.year >= start_year) & (ds[time_coord].dt.year <= end_year)\n",
    "            climatology = ds.sel({time_coord: mask}).groupby(f'{time_coord}.dayofyear').mean(time_coord)\n",
    "            anomalies = ds.groupby(f'{time_coord}.dayofyear') - climatology\n",
    "            \n",
    "        return anomalies\n",
    "    \n",
    "    def plot_hovmoller(self, ds, variable, lat_band=None):\n",
    "        \"\"\"绘制霍夫莫勒图\"\"\"\n",
    "        if lat_band is not None:\n",
    "            # 在特定纬度带计算平均\n",
    "            data = ds[variable].sel(latitude=slice(*lat_band)).mean('latitude')\n",
    "        else:\n",
    "            data = ds[variable].mean('latitude')\n",
    "            \n",
    "        fig, ax = plt.subplots(figsize=(15, 10))\n",
    "        data.plot(\n",
    "            x='longitude',\n",
    "            y='valid_time',  # 使用valid_time\n",
    "            cmap='RdBu_r',\n",
    "            robust=True\n",
    "        )\n",
    "        plt.title(f'{variable} Hovmöller Diagram')\n",
    "        return fig\n",
    "    \n",
    "    def analyze_daily_cycle(self, ds, variable, lat=None, lon=None):\n",
    "        \"\"\"分析日变化\"\"\"\n",
    "        if lat is not None and lon is not None:\n",
    "            data = ds[variable].sel(latitude=lat, longitude=lon, method='nearest')\n",
    "        else:\n",
    "            data = ds[variable].mean(dim=['latitude', 'longitude'])\n",
    "        \n",
    "        # 获取每个时间点的小时\n",
    "        hours = pd.to_datetime(data.valid_time.values).hour\n",
    "        \n",
    "        # 按小时分组计算平均值\n",
    "        daily_cycle = data.groupby(hours).mean()\n",
    "        \n",
    "        # 绘制日变化\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        daily_cycle.plot(ax=ax)\n",
    "        plt.xlabel('Hour of Day')\n",
    "        plt.title(f'{variable} Daily Cycle')\n",
    "        return fig\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数示例用法\"\"\"\n",
    "    analyzer = ERA5Analyzer()\n",
    "    \n",
    "    # 加载数据\n",
    "    print(\"加载数据...\")\n",
    "    ds = analyzer.load_data('era5_2021_12.nc')\n",
    "    \n",
    "    # 1. 基本统计分析\n",
    "    print(\"\\n计算基本统计信息：\")\n",
    "    stats = analyzer.basic_statistics(ds)\n",
    "    print(stats)\n",
    "    \n",
    "    # 2. 空间分布图\n",
    "    print(\"\\n绘制空间分布图...\")\n",
    "    for var in ds.data_vars:\n",
    "        try:\n",
    "            fig = analyzer.plot_spatial_mean(ds, var, \n",
    "                title=f'Mean {var} Distribution (December 2021)')\n",
    "            plt.savefig(f'spatial_mean_{var}.png')\n",
    "            plt.close()\n",
    "            print(f\"已保存 {var} 的空间分布图\")\n",
    "        except Exception as e:\n",
    "            print(f\"绘制 {var} 的空间分布图时出错: {str(e)}\")\n",
    "    \n",
    "    # 3. 时间序列分析（柏林）\n",
    "    print(\"\\n绘制时间序列...\")\n",
    "    berlin_lat, berlin_lon = 52.52, 13.41\n",
    "    for var in ds.data_vars:\n",
    "        try:\n",
    "            fig = analyzer.plot_time_series(ds, var, \n",
    "                lat=berlin_lat, lon=berlin_lon)\n",
    "            plt.savefig(f'time_series_{var}_berlin.png')\n",
    "            plt.close()\n",
    "            print(f\"已保存 {var} 的时间序列图\")\n",
    "        except Exception as e:\n",
    "            print(f\"绘制 {var} 的时间序列图时出错: {str(e)}\")\n",
    "    \n",
    "    # 4. 日变化分析\n",
    "    print(\"\\n分析日变化...\")\n",
    "    for var in ds.data_vars:\n",
    "        try:\n",
    "            fig = analyzer.analyze_daily_cycle(ds, var)\n",
    "            plt.savefig(f'daily_cycle_{var}.png')\n",
    "            plt.close()\n",
    "            print(f\"已保存 {var} 的日变化图\")\n",
    "        except Exception as e:\n",
    "            print(f\"分析 {var} 的日变化时出错: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ERA5 data...\n",
      "Found 7 data files\n",
      "Calculating daily climatology and standard deviation...\n",
      "Processing monthly data and calculating anomalies...\n",
      "Data preprocessing completed successfully!\n",
      "\n",
      "Performing Incremental PCA analysis...\n",
      "Error in perform_incremental_pca: 'StandardScaler' object has no attribute 'partial_fit_transform'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'StandardScaler' object has no attribute 'partial_fit_transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 266\u001b[0m\n\u001b[0;32m    259\u001b[0m     np\u001b[39m.\u001b[39msavez(explorer\u001b[39m.\u001b[39moutput_dir \u001b[39m/\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpca_results.npz\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    260\u001b[0m              pca_result\u001b[39m=\u001b[39mpca_result,\n\u001b[0;32m    261\u001b[0m              explained_variance\u001b[39m=\u001b[39mexplained_variance,\n\u001b[0;32m    262\u001b[0m              components\u001b[39m=\u001b[39mcomponents,\n\u001b[0;32m    263\u001b[0m              variables\u001b[39m=\u001b[39mvariables)\n\u001b[0;32m    265\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 266\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[1], line 236\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    233\u001b[0m anomalies, dates \u001b[39m=\u001b[39m explorer\u001b[39m.\u001b[39mload_and_preprocess()\n\u001b[0;32m    235\u001b[0m \u001b[39m# 2. 执行增量式PCA分析\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m pca_result, explained_variance, components, variables, dates_array \u001b[39m=\u001b[39m explorer\u001b[39m.\u001b[39;49mperform_incremental_pca(\n\u001b[0;32m    237\u001b[0m     anomalies, dates, n_components\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m  \u001b[39m# 减少主成分数量和批大小\u001b[39;49;00m\n\u001b[0;32m    238\u001b[0m )\n\u001b[0;32m    240\u001b[0m \u001b[39m# 3. 可视化结果\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[39m# 解释方差比例\u001b[39;00m\n\u001b[0;32m    242\u001b[0m explorer\u001b[39m.\u001b[39mplot_explained_variance(explained_variance)\n",
      "Cell \u001b[1;32mIn[1], line 187\u001b[0m, in \u001b[0;36mERA5Explorer.perform_incremental_pca\u001b[1;34m(self, anomalies, dates, n_components, batch_size)\u001b[0m\n\u001b[0;32m    184\u001b[0m batch_data \u001b[39m=\u001b[39m data_np[start_idx:end_idx, :]\n\u001b[0;32m    186\u001b[0m \u001b[39m# 标准化\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m batch_data \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mpartial_fit_transform(batch_data)\n\u001b[0;32m    189\u001b[0m \u001b[39m# 增量式 PCA 拟合和变换\u001b[39;00m\n\u001b[0;32m    190\u001b[0m batch_result \u001b[39m=\u001b[39m ipca\u001b[39m.\u001b[39mpartial_fit_transform(batch_data)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'StandardScaler' object has no attribute 'partial_fit_transform'"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # 忽略警告信息\n",
    "\n",
    "# 设置matplotlib和seaborn的绘图样式\n",
    "plt.style.use('default')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "class ERA5Explorer:\n",
    "    def __init__(self, data_dir='era5_data', output_dir='analysis_results'):\n",
    "        \"\"\"\n",
    "        初始化数据探索器\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def load_and_preprocess(self):\n",
    "        \"\"\"加载并预处理ERA5数据\"\"\"\n",
    "        try:\n",
    "            print(\"Loading ERA5 data...\")\n",
    "            data_files = sorted(self.data_dir.glob('era5_*.nc'))\n",
    "            print(f\"Found {len(data_files)} data files\")\n",
    "\n",
    "            # 定义要处理的空间范围（例如，选择经度 0-50，纬度 30-60）\n",
    "            lon_min, lon_max = 0, 50\n",
    "            lat_min, lat_max = 30, 60\n",
    "\n",
    "            # 选择要处理的变量，减少变量数量（例如，只处理 't2m' 和 'tp'）\n",
    "            variables = ['t2m', 'tp']\n",
    "\n",
    "            # 准备列表来存储所有月份的异常值数据\n",
    "            anomalies_list = []\n",
    "            dates_list = []\n",
    "\n",
    "            # 存储 climatology 和 std 用于后续计算\n",
    "            climatology_dict = {}\n",
    "            std_dict = {}\n",
    "\n",
    "            # 首先遍历所有数据文件，计算每日的气候态和标准差\n",
    "            print(\"Calculating daily climatology and standard deviation...\")\n",
    "            daily_data_list = []\n",
    "\n",
    "            for file in data_files:\n",
    "                ds = xr.open_dataset(file)\n",
    "\n",
    "                # 选择感兴趣的区域和变量\n",
    "                ds = ds.sel(longitude=slice(lon_min, lon_max), latitude=slice(lat_max, lat_min))\n",
    "                ds = ds[variables]\n",
    "\n",
    "                # 计算日平均\n",
    "                ds_daily = ds.resample(valid_time='1D').mean()\n",
    "\n",
    "                # 将每日数据添加到列表\n",
    "                daily_data_list.append(ds_daily)\n",
    "\n",
    "            # 合并所有日平均数据\n",
    "            all_daily_data = xr.concat(daily_data_list, dim='valid_time')\n",
    "            all_daily_data = all_daily_data.sortby('valid_time')\n",
    "\n",
    "            # 处理缺失值，使用前向填充和后向填充\n",
    "            all_daily_data = all_daily_data.ffill(dim='valid_time').bfill(dim='valid_time')\n",
    "\n",
    "            # 计算气候态和标准差\n",
    "            climatology = all_daily_data.groupby('valid_time.dayofyear').mean('valid_time')\n",
    "            std_dev = all_daily_data.groupby('valid_time.dayofyear').std('valid_time')\n",
    "\n",
    "            # 现在逐月处理数据，计算异常值\n",
    "            print(\"Processing monthly data and calculating anomalies...\")\n",
    "            for file in data_files:\n",
    "                ds = xr.open_dataset(file)\n",
    "\n",
    "                # 选择感兴趣的区域和变量\n",
    "                ds = ds.sel(longitude=slice(lon_min, lon_max), latitude=slice(lat_max, lat_min))\n",
    "                ds = ds[variables]\n",
    "\n",
    "                # 计算日平均\n",
    "                ds_daily = ds.resample(valid_time='1D').mean()\n",
    "\n",
    "                # 处理缺失值，使用前向填充和后向填充\n",
    "                ds_daily = ds_daily.ffill(dim='valid_time').bfill(dim='valid_time')\n",
    "\n",
    "                # 确保日期有序\n",
    "                ds_daily = ds_daily.sortby('valid_time')\n",
    "\n",
    "                # 计算异常值\n",
    "                anomalies = []\n",
    "                dates = []\n",
    "                for time in ds_daily['valid_time'].values:\n",
    "                    day = pd.to_datetime(time).dayofyear\n",
    "\n",
    "                    # 获取对应的气候态和标准差\n",
    "                    clim = climatology.sel(dayofyear=day)\n",
    "                    std = std_dev.sel(dayofyear=day)\n",
    "\n",
    "                    # 避免除以零\n",
    "                    std = std.where(std != 0, np.nan)\n",
    "\n",
    "                    # 计算异常值\n",
    "                    anomaly = (ds_daily.sel(valid_time=time) - clim) / std\n",
    "                    anomalies.append(anomaly)\n",
    "                    dates.append(time)\n",
    "\n",
    "                # 将 anomalies 合并\n",
    "                anomalies_month = xr.concat(anomalies, dim='valid_time')\n",
    "                anomalies_month['valid_time'] = dates\n",
    "\n",
    "                # 添加到列表\n",
    "                anomalies_list.append(anomalies_month)\n",
    "                dates_list.extend(dates)\n",
    "\n",
    "            # 合并所有月份的异常值数据\n",
    "            anomalies = xr.concat(anomalies_list, dim='valid_time')\n",
    "            anomalies = anomalies.sortby('valid_time')\n",
    "\n",
    "            print(\"Data preprocessing completed successfully!\")\n",
    "            return anomalies, dates_list\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in load_and_preprocess: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def perform_incremental_pca(self, anomalies, dates, n_components=3, batch_size=5):\n",
    "        \"\"\"\n",
    "        执行增量式PCA分析\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"\\nPerforming Incremental PCA analysis...\")\n",
    "\n",
    "            # 准备数据\n",
    "            variables = list(anomalies.data_vars)\n",
    "            data_arrays = []\n",
    "\n",
    "            for var in variables:\n",
    "                if var in anomalies:\n",
    "                    # 将空间维度展平，并重置索引\n",
    "                    flat_data = anomalies[var].stack(spatial=['latitude', 'longitude'])\n",
    "                    flat_data = flat_data.reset_index('spatial')\n",
    "                    data_arrays.append(flat_data)\n",
    "\n",
    "            # 合并所有变量\n",
    "            combined_data = xr.concat(data_arrays, dim='variable')\n",
    "\n",
    "            # 将变量和空间维度合并为特征维度\n",
    "            combined_data = combined_data.stack(features=['variable', 'spatial'])\n",
    "\n",
    "            # 确保数据维度为 (time, features)\n",
    "            combined_data = combined_data.transpose('valid_time', 'features')\n",
    "\n",
    "            # 填充NaN值\n",
    "            combined_data = combined_data.fillna(0)\n",
    "\n",
    "            # 将数据转换为 NumPy 数组\n",
    "            data_np = combined_data.values\n",
    "\n",
    "            # 创建增量式 PCA 对象\n",
    "            ipca = IncrementalPCA(n_components=n_components)\n",
    "\n",
    "            # 创建标准化器\n",
    "            scaler = StandardScaler()\n",
    "\n",
    "            # 获取样本数量\n",
    "            n_samples = data_np.shape[0]\n",
    "\n",
    "            # 计算批次数量\n",
    "            n_batches = n_samples // batch_size + (n_samples % batch_size != 0)\n",
    "\n",
    "            # 逐批次处理数据\n",
    "            pca_result = []\n",
    "            dates_array = np.array(dates)\n",
    "            for i in range(n_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i + 1) * batch_size, n_samples)\n",
    "\n",
    "                # 提取当前批次的数据\n",
    "                batch_data = data_np[start_idx:end_idx, :]\n",
    "\n",
    "                # 标准化\n",
    "                batch_data = scaler.partial_fit_transform(batch_data)\n",
    "\n",
    "                # 增量式 PCA 拟合和变换\n",
    "                batch_result = ipca.partial_fit_transform(batch_data)\n",
    "\n",
    "                pca_result.append(batch_result)\n",
    "\n",
    "                print(f\"Processed batch {i + 1}/{n_batches}\")\n",
    "\n",
    "            # 将所有批次的结果合并\n",
    "            pca_result = np.concatenate(pca_result, axis=0)\n",
    "\n",
    "            # 获取解释方差比例和主成分\n",
    "            explained_variance_ratio = ipca.explained_variance_ratio_\n",
    "            components = ipca.components_\n",
    "\n",
    "            return pca_result, explained_variance_ratio, components, variables, dates_array\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in perform_incremental_pca: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def plot_explained_variance(self, explained_variance_ratio):\n",
    "        \"\"\"绘制解释方差比例\"\"\"\n",
    "        # ...（保持不变）\n",
    "\n",
    "    def plot_time_evolution(self, pca_result, dates, n_components=3):\n",
    "        \"\"\"绘制主成分的时间演变\"\"\"\n",
    "        # ...（保持不变）\n",
    "                    \n",
    "    def analyze_seasonal_variation(self, pca_result, dates, n_components=3):\n",
    "        \"\"\"分析季节性变化\"\"\"\n",
    "        # ...（保持不变）\n",
    "\n",
    "    def plot_component_patterns(self, components, variables, anomalies, n_components=3):\n",
    "        \"\"\"绘制主成分模式（平均加载值）\"\"\"\n",
    "        # ...（保持不变）\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    explorer = ERA5Explorer(\n",
    "        data_dir='G:/workflow0822/githubclimate/paper_ERC23_code/era5_data',\n",
    "        output_dir='analysis_results'\n",
    "    )\n",
    "\n",
    "    # 1. 加载和预处理数据\n",
    "    anomalies, dates = explorer.load_and_preprocess()\n",
    "\n",
    "    # 2. 执行增量式PCA分析\n",
    "    pca_result, explained_variance, components, variables, dates_array = explorer.perform_incremental_pca(\n",
    "        anomalies, dates, n_components=3, batch_size=5  # 减少主成分数量和批大小\n",
    "    )\n",
    "\n",
    "    # 3. 可视化结果\n",
    "    # 解释方差比例\n",
    "    explorer.plot_explained_variance(explained_variance)\n",
    "\n",
    "    # 主成分模式\n",
    "    explorer.plot_component_patterns(components, variables, anomalies, n_components=3)\n",
    "\n",
    "    # 时间演变\n",
    "    explorer.plot_time_evolution(pca_result, dates_array, n_components=3)\n",
    "\n",
    "    # 季节性变化\n",
    "    explorer.analyze_seasonal_variation(pca_result, dates_array, n_components=3)\n",
    "\n",
    "    # 4. 打印主要发现\n",
    "    print(\"\\nKey Findings:\")\n",
    "    print(\"-------------\")\n",
    "    print(f\"Total explained variance by first 3 PCs: {np.sum(explained_variance)*100:.2f}%\")\n",
    "\n",
    "    # 保存结果\n",
    "    np.savez(explorer.output_dir / 'pca_results.npz',\n",
    "             pca_result=pca_result,\n",
    "             explained_variance=explained_variance,\n",
    "             components=components,\n",
    "             variables=variables)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory optimized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ERA5 data...\n",
      "Found 7 data files\n",
      "Processing file: era5_2020_01.nc\n",
      "Data preprocessing completed successfully!\n",
      "\n",
      "Performing PCA analysis...\n",
      "Top 2 PCs explain 26.2% of total variance\n",
      "\n",
      "Key Findings:\n",
      "-------------\n",
      "Total explained variance by first 2 PCs: 26.17%\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # 忽略警告信息\n",
    "\n",
    "# 设置matplotlib和seaborn的绘图样式\n",
    "plt.style.use('default')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "class ERA5Explorer:\n",
    "    def __init__(self, data_dir='era5_data', output_dir='analysis_results'):\n",
    "        \"\"\"\n",
    "        初始化数据探索器\n",
    "        \"\"\"\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def load_and_preprocess(self):\n",
    "        \"\"\"加载并预处理ERA5数据\"\"\"\n",
    "        try:\n",
    "            print(\"Loading ERA5 data...\")\n",
    "            data_files = sorted(self.data_dir.glob('era5_*.nc'))\n",
    "            print(f\"Found {len(data_files)} data files\")\n",
    "\n",
    "            # **仅处理一个文件**\n",
    "            if not data_files:\n",
    "                raise FileNotFoundError(\"No data files found in the specified directory.\")\n",
    "            file = data_files[0]\n",
    "            print(f\"Processing file: {file.name}\")\n",
    "\n",
    "            # 定义要处理的空间范围（可以进一步缩小以减少数据量）\n",
    "            lon_min, lon_max = 0, 50\n",
    "            lat_min, lat_max = 30, 60\n",
    "\n",
    "            # 选择要处理的变量，减少变量数量\n",
    "            variables = ['t2m', 'tp']\n",
    "\n",
    "            # 打开数据集\n",
    "            ds = xr.open_dataset(file)\n",
    "\n",
    "            # 选择感兴趣的区域和变量\n",
    "            ds = ds.sel(longitude=slice(lon_min, lon_max), latitude=slice(lat_max, lat_min))\n",
    "            ds = ds[variables]\n",
    "\n",
    "            # 计算日平均\n",
    "            ds_daily = ds.resample(valid_time='1D').mean()\n",
    "\n",
    "            # 处理缺失值，使用前向填充和后向填充\n",
    "            ds_daily = ds_daily.ffill(dim='valid_time').bfill(dim='valid_time')\n",
    "\n",
    "            # 确保日期有序\n",
    "            ds_daily = ds_daily.sortby('valid_time')\n",
    "\n",
    "            # 计算气候态和标准差（由于只有一个月的数据，这里直接计算平均值和标准差）\n",
    "            climatology = ds_daily.mean('valid_time')\n",
    "            std_dev = ds_daily.std('valid_time')\n",
    "\n",
    "            # 计算异常值\n",
    "            anomalies = (ds_daily - climatology) / std_dev\n",
    "\n",
    "            # 避免出现 NaN 和 Inf\n",
    "            anomalies = anomalies.fillna(0)\n",
    "            anomalies = anomalies.where(np.isfinite(anomalies), 0)\n",
    "\n",
    "            # 重置索引（修复错误）\n",
    "            if 'dayofyear' in anomalies.indexes:\n",
    "                anomalies = anomalies.reset_index('dayofyear')\n",
    "\n",
    "            # 确保 'dayofyear' 不在坐标中\n",
    "            anomalies = anomalies.reset_coords(drop=True)\n",
    "\n",
    "            dates_list = ds_daily['valid_time'].values\n",
    "\n",
    "            print(\"Data preprocessing completed successfully!\")\n",
    "            return anomalies, dates_list\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in load_and_preprocess: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def perform_pca(self, anomalies, dates, n_components=2):\n",
    "        \"\"\"\n",
    "        执行PCA分析\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\"\\nPerforming PCA analysis...\")\n",
    "\n",
    "            # 准备数据\n",
    "            variables = list(anomalies.data_vars)\n",
    "            data_arrays = []\n",
    "\n",
    "            for var in variables:\n",
    "                if var in anomalies:\n",
    "                    # 将空间维度展平，并重置索引\n",
    "                    flat_data = anomalies[var].stack(spatial=['latitude', 'longitude'])\n",
    "                    flat_data = flat_data.reset_index('spatial')\n",
    "                    data_arrays.append(flat_data)\n",
    "\n",
    "            # 合并所有变量\n",
    "            combined_data = xr.concat(data_arrays, dim='variable')\n",
    "\n",
    "            # 将变量和空间维度合并为特征维度\n",
    "            combined_data = combined_data.stack(features=['variable', 'spatial'])\n",
    "\n",
    "            # 确保数据维度为 (valid_time, features)\n",
    "            combined_data = combined_data.transpose('valid_time', 'features')\n",
    "\n",
    "            # 填充NaN值\n",
    "            combined_data = combined_data.fillna(0)\n",
    "\n",
    "            # 将数据转换为 NumPy 数组\n",
    "            data_np = combined_data.values\n",
    "\n",
    "            # 标准化数据\n",
    "            scaler = StandardScaler()\n",
    "            data_scaled = scaler.fit_transform(data_np)\n",
    "\n",
    "            # 执行PCA\n",
    "            pca = PCA(n_components=n_components)\n",
    "            pca_result = pca.fit_transform(data_scaled)\n",
    "\n",
    "            # 获取解释方差比例和主成分\n",
    "            explained_variance_ratio = pca.explained_variance_ratio_\n",
    "            components = pca.components_\n",
    "\n",
    "            dates_array = np.array(dates)\n",
    "\n",
    "            return pca_result, explained_variance_ratio, components, variables, dates_array\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in perform_pca: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def plot_explained_variance(self, explained_variance_ratio):\n",
    "        \"\"\"绘制解释方差比例\"\"\"\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            bars = plt.bar(range(1, len(explained_variance_ratio) + 1), \n",
    "                           explained_variance_ratio * 100)\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                         f'{height:.1f}%',\n",
    "                         ha='center', va='bottom')\n",
    "            plt.xlabel('Principal Component')\n",
    "            plt.ylabel('Explained Variance Ratio (%)')\n",
    "            plt.title('Explained Variance Ratio by Principal Components')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            # 添加累计解释方差\n",
    "            cumsum = np.cumsum(explained_variance_ratio) * 100\n",
    "            ax2 = plt.gca().twinx()\n",
    "            ax2.plot(range(1, len(explained_variance_ratio) + 1), cumsum, \n",
    "                     'r-', label='Cumulative Variance')\n",
    "            ax2.set_ylabel('Cumulative Explained Variance (%)')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.output_dir / 'explained_variance.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            print(f\"Top {len(explained_variance_ratio)} PCs explain {cumsum[-1]:.1f}% of total variance\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in plot_explained_variance: {str(e)}\")\n",
    "\n",
    "    def plot_time_evolution(self, pca_result, dates, n_components=2):\n",
    "        \"\"\"绘制主成分的时间演变\"\"\"\n",
    "        try:\n",
    "            plt.figure(figsize=(15, 8))\n",
    "            # 转换dates为datetime对象\n",
    "            if not isinstance(dates[0], pd.Timestamp):\n",
    "                dates = pd.to_datetime(dates)\n",
    "            # 绘制每个主成分的时间序列\n",
    "            for i in range(min(n_components, pca_result.shape[1])):\n",
    "                plt.plot(dates, pca_result[:, i], \n",
    "                         label=f'PC{i+1}', alpha=0.7)\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('PC Value')\n",
    "            plt.title('Time Evolution of Principal Components')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.output_dir / 'pc_time_evolution.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error in plot_time_evolution: {str(e)}\")\n",
    "                \n",
    "    def plot_component_patterns(self, components, variables, anomalies, n_components=2):\n",
    "        \"\"\"绘制主成分模式（平均加载值）\"\"\"\n",
    "        try:\n",
    "            # 由于组件是展平的特征，需要重新reshape\n",
    "            n_features = components.shape[1]\n",
    "            n_variables = len(variables)\n",
    "            n_spatial = n_features // n_variables\n",
    "\n",
    "            fig, axes = plt.subplots(n_components, 1, figsize=(12, 4 * n_components))\n",
    "            if n_components == 1:\n",
    "                axes = [axes]\n",
    "            for i, ax in enumerate(axes):\n",
    "                # 提取第 i 个主成分的加载值\n",
    "                component = components[i]\n",
    "                # 将加载值按变量和空间维度重新reshape\n",
    "                component_reshaped = component.reshape((n_variables, n_spatial))\n",
    "                mean_loadings = component_reshaped.mean(axis=1)\n",
    "                ax.bar(variables, mean_loadings)\n",
    "                ax.set_title(f'PC{i+1} Mean Loadings')\n",
    "                ax.set_ylabel('Loading')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(self.output_dir / 'component_patterns.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Error in plot_component_patterns: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    explorer = ERA5Explorer(\n",
    "        data_dir='G:/workflow0822/githubclimate/paper_ERC23_code/era5_data',\n",
    "        output_dir='analysis_results'\n",
    "    )\n",
    "\n",
    "    # 1. 加载和预处理数据\n",
    "    anomalies, dates = explorer.load_and_preprocess()\n",
    "\n",
    "    # 2. 执行PCA分析\n",
    "    pca_result, explained_variance, components, variables, dates_array = explorer.perform_pca(\n",
    "        anomalies, dates, n_components=2\n",
    "    )\n",
    "\n",
    "    # 3. 可视化结果\n",
    "    # 解释方差比例\n",
    "    explorer.plot_explained_variance(explained_variance)\n",
    "\n",
    "    # 主成分模式\n",
    "    explorer.plot_component_patterns(components, variables, anomalies, n_components=2)\n",
    "\n",
    "    # 时间演变\n",
    "    explorer.plot_time_evolution(pca_result, dates_array, n_components=2)\n",
    "\n",
    "    # 4. 打印主要发现\n",
    "    print(\"\\nKey Findings:\")\n",
    "    print(\"-------------\")\n",
    "    print(f\"Total explained variance by first 2 PCs: {np.sum(explained_variance)*100:.2f}%\")\n",
    "\n",
    "    # 保存结果\n",
    "    np.savez(explorer.output_dir / 'pca_results.npz',\n",
    "             pca_result=pca_result,\n",
    "             explained_variance=explained_variance,\n",
    "             components=components,\n",
    "             variables=variables)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-17 21:13:28,332 - SubseasonalPredictor - INFO - 加载数据...\n",
      "2024-11-17 21:13:29,290 - SubseasonalPredictor - INFO - 验证数据质量...\n",
      "2024-11-17 21:13:29,294 - SubseasonalPredictor - WARNING - 发现时间不连续: [363 607]\n",
      "2024-11-17 21:13:31,926 - SubseasonalPredictor - WARNING - tp存在异常值，位置: (array([  0,   0,   0, ..., 851, 851, 851], dtype=int64), array([  5,   5,   5, ..., 155, 156, 157], dtype=int64), array([ 86,  87,  88, ..., 176, 320, 320], dtype=int64))\n",
      "2024-11-17 21:13:34,682 - SubseasonalPredictor - WARNING - t2m存在异常值，位置: (array([114, 114, 114, 114, 114, 114, 114, 114, 114, 115, 115, 115, 115,\n",
      "       115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115,\n",
      "       115, 115, 115, 116, 116, 116, 116, 116, 128, 128, 128, 128, 128,\n",
      "       128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,\n",
      "       128, 128, 128, 128, 128, 128, 128, 128, 577, 577, 577, 577, 577,\n",
      "       577, 577, 578, 578, 578, 578, 578, 578, 578, 578, 578, 578, 578,\n",
      "       578, 578, 578, 578, 578, 578, 578, 578, 578, 578, 579, 579, 579,\n",
      "       579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579,\n",
      "       579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579,\n",
      "       579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579,\n",
      "       579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579,\n",
      "       579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579,\n",
      "       579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579,\n",
      "       579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579,\n",
      "       579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 579, 580,\n",
      "       580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580,\n",
      "       580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580,\n",
      "       580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580,\n",
      "       580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580,\n",
      "       580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580,\n",
      "       580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580,\n",
      "       580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580,\n",
      "       580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580,\n",
      "       580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580,\n",
      "       580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580,\n",
      "       580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580,\n",
      "       580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580,\n",
      "       580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580,\n",
      "       580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580, 580,\n",
      "       580, 580, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581,\n",
      "       581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581,\n",
      "       581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581,\n",
      "       581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581,\n",
      "       581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581,\n",
      "       581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581,\n",
      "       581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581,\n",
      "       581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581,\n",
      "       581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581,\n",
      "       581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581,\n",
      "       581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581,\n",
      "       581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581, 581,\n",
      "       582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
      "       582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
      "       582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
      "       582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
      "       582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
      "       582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 582,\n",
      "       582, 582, 582, 582, 582, 582, 582, 582, 582, 582, 583, 583, 583,\n",
      "       583, 583, 583, 583, 583, 583, 583, 583, 583, 583, 583, 583, 583,\n",
      "       583, 583, 583, 583, 583, 583, 583, 583, 583, 583, 583, 583, 583,\n",
      "       583, 583, 583, 583, 583, 583, 583, 583, 583, 583, 583, 583, 583,\n",
      "       583, 583, 583, 583, 583, 583, 583, 583, 583, 583, 583, 583, 583,\n",
      "       583, 583, 583, 583, 583, 583, 583, 583, 583, 583, 583, 583, 583,\n",
      "       583, 583, 583, 583, 583, 583, 583, 583, 583, 583, 583, 583, 583,\n",
      "       583, 583, 583, 583, 583, 583, 583, 583, 583, 583, 583, 583, 583,\n",
      "       583, 583, 583, 583, 584, 584, 584, 584, 584, 584, 584, 584, 584,\n",
      "       584, 584, 584, 584, 584, 584, 584, 584, 584, 584, 584, 584, 584,\n",
      "       584, 584, 584, 584, 584, 584, 584, 584, 584, 584, 584, 584, 584,\n",
      "       584, 584, 584, 584, 584, 584, 584, 584, 584, 584, 584, 584, 584,\n",
      "       584, 584, 584, 584, 584, 584, 584, 584, 584, 584, 584, 584, 584,\n",
      "       584, 584, 584, 584, 584, 584, 584, 584, 584, 584, 584, 585, 585,\n",
      "       585, 585, 585, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601,\n",
      "       601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601, 601,\n",
      "       601, 601, 601, 601, 601, 601, 601, 602, 602, 602, 602, 602, 602,\n",
      "       602, 602, 602, 602, 602, 602, 602, 602, 602, 603, 603, 603, 603,\n",
      "       603, 603, 604, 604], dtype=int64), array([ 9,  9, 10, 10, 10, 10, 10, 10, 11,  9,  9,  9, 10, 10, 10, 10, 10,\n",
      "       10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 12,  8,  9,  9,  9,  9,\n",
      "       13, 13, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 16, 16, 16, 16,\n",
      "       16, 16, 16, 17, 17, 17, 17, 17, 17, 21, 21, 21, 21, 22, 22, 22,  8,\n",
      "        8,  9,  9, 19, 19, 19, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21,\n",
      "       21, 21, 28,  8,  9,  9,  9, 10, 10, 10, 11, 11, 11, 11, 11, 11, 12,\n",
      "       12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13,\n",
      "       13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 17, 17,\n",
      "       17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19,\n",
      "       19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "       20, 20, 21, 21, 21, 21, 21, 22, 22, 25, 25, 27, 27, 28, 28, 28, 28,\n",
      "       29, 29, 29, 30, 30, 30, 31,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,\n",
      "       10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "       11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "       12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "       13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
      "       14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "       15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "       17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
      "       18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20,\n",
      "       20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 25, 25, 26,\n",
      "       26, 26, 27, 27, 27,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9, 10,\n",
      "       10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
      "       11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
      "       12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
      "       13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15,\n",
      "       15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17,\n",
      "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "       18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19,\n",
      "       19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "       20, 21, 21, 21, 21, 25,  8,  8,  8,  8,  9,  9,  9,  9, 12, 12, 12,\n",
      "       12, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 15, 15, 16, 16, 16, 16,\n",
      "       16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "       18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19,\n",
      "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20,\n",
      "       20, 20, 20, 20, 21, 21, 23, 24, 24,  8, 11, 12, 12, 12, 12, 12, 13,\n",
      "       13, 13, 13, 14, 14, 14, 14, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17,\n",
      "       17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18,\n",
      "       18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19,\n",
      "       19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20,\n",
      "       20, 20, 20, 20, 20, 12, 12, 12, 12, 13, 13, 13, 13, 13, 14, 14, 14,\n",
      "       14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
      "       15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n",
      "       16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "       18, 18, 18, 18, 18, 18, 18, 18, 18, 16, 16, 16, 17, 17, 11, 15, 15,\n",
      "       15, 15, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "       18, 18, 18, 18, 18, 19, 19, 19, 19, 20, 16, 17, 17, 17, 17, 17, 18,\n",
      "       18, 22, 22, 22, 23, 23, 23, 23, 17, 17, 17, 22, 22, 22, 17, 17],\n",
      "      dtype=int64), array([311, 312, 306, 307, 308, 309, 310, 311, 308, 312, 313, 314, 312,\n",
      "       313, 314, 315, 316, 317, 318, 319, 320, 314, 315, 316, 317, 318,\n",
      "       319, 320, 320, 314, 311, 312, 313, 314, 261, 262, 261, 262, 263,\n",
      "       264, 265, 266, 259, 260, 261, 262, 263, 257, 258, 259, 260, 261,\n",
      "       262, 263, 258, 259, 260, 261, 262, 263, 308, 309, 310, 311, 309,\n",
      "       310, 311, 319, 320, 319, 320, 309, 310, 312, 306, 307, 308, 309,\n",
      "       310, 311, 304, 305, 306, 307, 308, 309, 310, 318, 320, 318, 319,\n",
      "       320, 318, 319, 320, 315, 316, 317, 318, 319, 320, 310, 311, 312,\n",
      "       313, 314, 315, 316, 317, 318, 319, 320, 310, 311, 312, 313, 314,\n",
      "       315, 316, 317, 318, 319, 320, 309, 310, 314, 315, 316, 317, 318,\n",
      "       319, 320, 319, 320, 307, 308, 309, 310, 311, 312, 313, 314, 303,\n",
      "       304, 305, 306, 307, 308, 309, 310, 311, 312, 302, 303, 304, 305,\n",
      "       306, 307, 308, 309, 310, 311, 302, 303, 304, 305, 306, 307, 308,\n",
      "       309, 310, 311, 306, 307, 308, 309, 310, 310, 311, 317, 318, 318,\n",
      "       320, 317, 318, 319, 320, 318, 319, 320, 318, 319, 320, 319, 316,\n",
      "       317, 318, 319, 320, 316, 317, 318, 319, 320, 314, 315, 316, 317,\n",
      "       318, 319, 320, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315,\n",
      "       316, 317, 318, 319, 320, 302, 303, 304, 305, 306, 307, 308, 309,\n",
      "       310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 302, 303,\n",
      "       304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316,\n",
      "       317, 318, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313,\n",
      "       314, 315, 316, 317, 318, 319, 303, 304, 305, 306, 307, 308, 309,\n",
      "       310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 302, 303,\n",
      "       304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316,\n",
      "       317, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313,\n",
      "       314, 315, 316, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310,\n",
      "       311, 312, 313, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310,\n",
      "       311, 312, 313, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312,\n",
      "       313, 307, 308, 309, 310, 311, 312, 317, 318, 317, 318, 319, 317,\n",
      "       318, 319, 316, 317, 318, 319, 320, 315, 316, 317, 318, 319, 320,\n",
      "       315, 316, 317, 318, 319, 320, 306, 307, 308, 309, 310, 311, 312,\n",
      "       313, 314, 315, 316, 317, 318, 319, 320, 288, 300, 301, 302, 303,\n",
      "       304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 289, 290,\n",
      "       291, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312,\n",
      "       313, 290, 291, 302, 303, 304, 305, 306, 307, 308, 309, 310, 290,\n",
      "       291, 302, 303, 304, 305, 306, 307, 308, 301, 302, 303, 304, 305,\n",
      "       306, 307, 308, 309, 310, 311, 300, 301, 302, 303, 304, 305, 306,\n",
      "       307, 308, 309, 310, 311, 312, 313, 314, 315, 299, 300, 301, 302,\n",
      "       303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 301, 302,\n",
      "       303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 304, 305,\n",
      "       306, 307, 308, 309, 310, 311, 312, 313, 310, 311, 312, 313, 318,\n",
      "       317, 318, 319, 320, 317, 318, 319, 320, 287, 288, 289, 290, 286,\n",
      "       287, 288, 289, 290, 291, 288, 289, 290, 291, 290, 291, 303, 304,\n",
      "       305, 306, 307, 308, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
      "       310, 311, 312, 313, 314, 315, 299, 300, 301, 302, 303, 304, 305,\n",
      "       306, 307, 308, 309, 310, 311, 312, 313, 301, 302, 303, 304, 305,\n",
      "       306, 307, 308, 309, 310, 311, 312, 313, 304, 305, 306, 307, 308,\n",
      "       309, 310, 311, 312, 313, 311, 312, 320, 319, 320, 319, 280, 287,\n",
      "       288, 289, 290, 291, 287, 288, 289, 290, 288, 289, 290, 291, 290,\n",
      "       291, 292, 292, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310,\n",
      "       311, 312, 313, 314, 315, 316, 296, 297, 298, 299, 300, 301, 302,\n",
      "       303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315,\n",
      "       316, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "       308, 309, 310, 311, 312, 313, 314, 300, 301, 302, 303, 304, 305,\n",
      "       306, 307, 308, 309, 310, 311, 312, 313, 305, 306, 307, 308, 309,\n",
      "       310, 311, 312, 313, 287, 288, 289, 290, 287, 288, 289, 290, 291,\n",
      "       288, 289, 290, 291, 306, 307, 308, 309, 310, 290, 291, 292, 301,\n",
      "       302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314,\n",
      "       299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
      "       312, 313, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312,\n",
      "       313, 314, 305, 306, 307, 308, 309, 310, 311, 312, 313, 313, 314,\n",
      "       315, 313, 314, 319, 311, 312, 313, 314, 310, 311, 312, 313, 314,\n",
      "       315, 307, 308, 309, 311, 312, 313, 314, 315, 316, 306, 307, 311,\n",
      "       312, 313, 304, 305, 312, 313, 312, 314, 312, 313, 314, 315, 316,\n",
      "       311, 312, 311, 312, 313, 311, 312, 313, 314, 313, 314, 315, 311,\n",
      "       312, 313, 314, 315], dtype=int64))\n",
      "2024-11-17 21:13:37,873 - SubseasonalPredictor - WARNING - msl存在异常值，位置: (array([ 27,  27,  27, ..., 187, 187, 187], dtype=int64), array([11, 11, 11, ..., 42, 42, 42], dtype=int64), array([33, 34, 35, ..., 29, 30, 31], dtype=int64))\n",
      "2024-11-17 21:13:40,548 - SubseasonalPredictor - WARNING - u10存在异常值，位置: (array([ 10,  10,  10,  10,  10,  32,  32,  32,  32,  32,  32,  50,  50,\n",
      "        50,  50,  50,  50,  50,  50,  50,  50,  50,  50,  50,  50,  50,\n",
      "        50,  50,  50,  50,  50,  50,  50,  50,  50,  50,  50,  50,  50,\n",
      "        50,  50,  50,  50,  50,  50,  50,  50,  50,  50,  50,  50,  50,\n",
      "        50,  50,  50,  50,  50,  50,  50,  50,  50,  50,  50,  50,  50,\n",
      "        50,  50,  50,  50,  51,  51,  51,  51,  51,  51,  51,  51,  51,\n",
      "        51,  51,  51,  51,  51,  51,  51,  51,  51,  51,  51,  51,  51,\n",
      "        51,  51,  51,  51,  51,  51,  51,  51,  51,  51,  51,  51,  51,\n",
      "        51,  51,  51,  51,  51,  51,  51,  51,  51,  51,  51,  51,  51,\n",
      "        51,  51,  51,  51,  51,  51,  51,  51,  51,  51,  51,  51,  51,\n",
      "        51,  51,  51,  51,  51,  51,  51,  51,  51,  51,  51,  51,  51,\n",
      "        51,  51,  51,  51,  51,  51,  51,  51,  51,  51,  52,  52, 177,\n",
      "       177, 177, 177, 177, 177, 177, 177, 177, 177, 177, 177, 177, 177,\n",
      "       177, 177, 177, 177, 177, 177, 177, 177, 177, 177, 177, 177, 177,\n",
      "       178, 178, 178, 178, 178, 178, 178, 178, 178, 178, 178, 178, 178,\n",
      "       178, 178, 178, 178, 178, 179, 179, 179, 179, 179, 179, 179, 179,\n",
      "       179, 179, 181, 181, 181, 181, 181, 181, 181, 181, 236, 237, 237,\n",
      "       237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237, 237,\n",
      "       237, 237, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239,\n",
      "       239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239,\n",
      "       239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239, 239,\n",
      "       239, 239, 239, 239, 240, 240, 240, 240, 240, 240, 240, 240, 240,\n",
      "       240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240,\n",
      "       240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240,\n",
      "       240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240, 240,\n",
      "       240, 240, 240, 240, 240, 241, 241, 241, 241, 241, 241, 241, 241,\n",
      "       241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241,\n",
      "       241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241,\n",
      "       241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241,\n",
      "       241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241, 241,\n",
      "       241, 241, 248, 284, 586, 586, 586, 586, 586, 753, 753, 753, 753,\n",
      "       753, 753, 753, 753, 753, 753, 753, 753, 753, 753, 753, 753, 753,\n",
      "       753, 753, 753, 753, 753, 753, 753, 753, 753, 753, 753, 753, 754],\n",
      "      dtype=int64), array([17, 17, 17, 17, 18, 90, 91, 91, 92, 92, 92, 62, 63, 63, 63, 63, 64,\n",
      "       64, 64, 64, 64, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 66, 66,\n",
      "       66, 66, 66, 66, 66, 66, 66, 66, 66, 67, 67, 67, 67, 67, 67, 67, 67,\n",
      "       67, 67, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 69, 69, 69, 69, 69,\n",
      "       69, 48, 49, 49, 49, 49, 49, 50, 50, 50, 50, 50, 50, 51, 51, 51, 51,\n",
      "       51, 51, 51, 52, 52, 52, 52, 52, 52, 52, 53, 53, 53, 53, 53, 53, 53,\n",
      "       53, 53, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 55, 55, 55,\n",
      "       55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 56, 56, 56, 56, 56,\n",
      "       56, 56, 56, 56, 56, 56, 56, 56, 56, 57, 57, 57, 57, 57, 57, 57, 57,\n",
      "       47, 47, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "       28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 13, 14, 14, 14, 14,\n",
      "       14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 19, 13, 13, 13, 14,\n",
      "       14, 14, 14, 14, 14, 14, 55, 55, 56, 56, 56, 56, 57, 57, 64, 64, 64,\n",
      "       65, 65, 65, 65, 65, 66, 66, 66, 66, 66, 67, 67, 67, 67, 67, 30, 30,\n",
      "       30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31, 31, 31, 31, 31, 31, 31,\n",
      "       31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33,\n",
      "       33, 33, 33, 33, 33, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29,\n",
      "       29, 29, 29, 29, 29, 29, 29, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "       30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 31, 31,\n",
      "       31, 31, 31, 31, 31, 31, 31, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27,\n",
      "       27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28,\n",
      "       28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29,\n",
      "       29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29,\n",
      "       29, 22, 62, 59, 60, 60, 60, 60, 74, 74, 74, 74, 75, 75, 75, 75, 75,\n",
      "       75, 76, 76, 76, 76, 76, 76, 76, 77, 77, 77, 77, 77, 77, 77, 78, 78,\n",
      "       78, 78, 78, 58], dtype=int64), array([85, 86, 87, 88, 87,  0,  0,  1,  0,  1,  2,  3,  3,  4,  5,  6,  3,\n",
      "        4,  5,  6,  7,  8,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,  5,  6,\n",
      "        7,  8,  9, 10, 11, 12, 13, 14, 15,  7,  8,  9, 10, 11, 12, 13, 14,\n",
      "       15, 16,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 11, 12, 13, 14, 15,\n",
      "       16,  2,  2,  3,  4,  5,  6,  5,  6,  7,  8,  9, 10,  8,  9, 10, 11,\n",
      "       12, 13, 14, 11, 12, 13, 14, 15, 16, 17, 14, 15, 16, 17, 18, 19, 20,\n",
      "       21, 22, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 19, 20, 21,\n",
      "       22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 23, 24, 25, 26, 27,\n",
      "       28, 29, 30, 31, 32, 33, 34, 35, 36, 29, 30, 31, 32, 33, 34, 35, 36,\n",
      "        7,  8,  7, 10, 11,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11,\n",
      "        0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11,  0,  0,  1,  2,  3,\n",
      "        4,  5,  6,  7,  8,  9,  0,  1,  2,  3,  4,  5, 30, 18, 19, 20,  0,\n",
      "        1,  2,  5,  6,  7,  8,  0,  1,  0,  1,  2,  3,  0,  1,  5,  8,  9,\n",
      "        8,  9, 10, 11, 12, 10, 11, 12, 13, 14, 13, 14, 15, 16, 17, 44, 48,\n",
      "       49, 50, 51, 55, 56, 57, 58, 59, 60, 57, 58, 59, 60, 61, 62, 63, 64,\n",
      "       65, 66, 67, 68, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 50,\n",
      "       51, 59, 60, 61, 62, 60, 61, 62, 63, 64, 65, 54, 55, 56, 57, 58, 59,\n",
      "       60, 61, 62, 63, 64, 65, 66, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45,\n",
      "       46, 47, 48, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 35, 36,\n",
      "       37, 38, 44, 45, 57, 58, 59, 44, 41, 42, 43, 44, 45, 46, 47, 48, 49,\n",
      "       50, 51, 52, 53, 54, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,\n",
      "       48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 36, 37, 39,\n",
      "       40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 59, 60, 61,\n",
      "       62, 60,  5,  1,  1,  2,  3,  4, 23, 24, 25, 26, 23, 24, 25, 26, 27,\n",
      "       28, 24, 25, 26, 27, 28, 29, 30, 25, 26, 27, 28, 29, 30, 31, 26, 27,\n",
      "       28, 29, 30, 53], dtype=int64))\n",
      "2024-11-17 21:13:43,284 - SubseasonalPredictor - WARNING - v10存在异常值，位置: (array([  9,   9,   9,   9,   9,   9,   9,   9,  23,  45,  45,  45,  45,\n",
      "        45,  45,  45,  45,  45,  45,  45,  45,  45,  45,  45,  45,  45,\n",
      "        45,  45,  45,  45,  45,  45,  45,  45,  45,  45,  45,  45,  45,\n",
      "        45,  45,  45,  45,  45,  45,  45,  45,  45,  45,  45,  45,  45,\n",
      "        45,  45,  45,  45,  45,  45,  45,  45,  45,  45,  45,  45,  45,\n",
      "        45,  45,  45,  45,  45,  45,  45,  45,  45,  45,  45,  45,  45,\n",
      "        45,  45,  45,  45,  45,  45,  45,  45,  45,  45,  45,  45,  45,\n",
      "        45,  45,  45,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,\n",
      "        46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,\n",
      "        46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,\n",
      "        46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,\n",
      "        46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,\n",
      "        46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,\n",
      "        46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,\n",
      "        46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,\n",
      "        46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,\n",
      "        46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  46,  47,  47,\n",
      "        47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,\n",
      "        47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,\n",
      "        47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,\n",
      "        47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,  47,\n",
      "        47,  47,  47,  47,  49,  49,  49,  50,  50,  51,  51,  51,  55,\n",
      "        55,  55,  55, 156, 176, 176, 176, 176, 177, 177, 177, 177, 177,\n",
      "       177, 177, 177, 177, 177, 177, 177, 177, 177, 177, 177, 177, 177,\n",
      "       177, 177, 177, 177, 177, 177, 177, 177, 177, 177, 177, 177, 177,\n",
      "       177, 180, 180, 180, 180, 180, 181, 181, 181, 181, 181, 181, 181,\n",
      "       181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 181, 181,\n",
      "       181, 181, 181, 181, 181, 181, 181, 181, 181, 182, 182, 182, 182,\n",
      "       182, 182, 182, 182, 182, 182, 182, 182, 182, 182, 182, 182, 182,\n",
      "       182, 184, 184, 184, 188, 304, 304, 304, 436, 436, 436, 436, 436,\n",
      "       436, 436, 436, 436, 436, 436, 436, 436, 436, 436, 436, 436, 436,\n",
      "       436, 436, 436, 436, 436, 436, 436, 436, 436, 436, 436, 436, 436,\n",
      "       487, 488, 488, 488, 488, 488, 488, 488, 488, 488, 488, 488, 488,\n",
      "       488, 488, 488, 488, 488, 488, 488, 488, 488, 488, 488, 488, 488,\n",
      "       488, 488, 488, 488, 488, 488, 488, 488, 488, 488, 488, 488, 488,\n",
      "       488, 488, 488, 489, 489, 489, 489, 489, 489, 489, 489, 489, 489,\n",
      "       489, 489, 489, 489, 489, 489, 489, 489, 489, 489, 489, 489, 489,\n",
      "       489, 489, 489, 489, 489, 489, 489, 489, 489, 489, 489, 489, 489,\n",
      "       489, 489, 489, 489, 489, 489, 489, 489, 489, 489, 489, 489, 489,\n",
      "       489, 489, 489, 489, 489, 489, 489, 489, 489, 489, 489, 489, 489,\n",
      "       489, 489, 489, 489, 490, 490, 490, 490, 490, 490, 490, 490, 490,\n",
      "       490, 490, 490, 490, 490, 490, 490, 490, 490, 490, 490, 490, 490,\n",
      "       490, 490, 490, 493, 493, 493, 494, 494, 494, 494, 494, 494, 494,\n",
      "       495, 495, 495, 496, 496, 496, 496, 496, 496, 496, 496, 496, 496,\n",
      "       496, 496, 496, 496, 496, 496, 496, 496, 496, 496, 496, 496, 496,\n",
      "       496, 496, 496, 496, 496, 496, 496, 496, 496, 496, 496, 496, 496,\n",
      "       496, 496, 496, 496, 496, 496, 496, 496, 497, 497, 497, 497, 497,\n",
      "       497, 497, 497, 497, 497, 497, 497, 497, 497, 497, 497, 497, 497,\n",
      "       497, 497, 497, 497, 497, 497, 497, 497, 497, 497, 497, 497, 497,\n",
      "       497, 497, 497, 497, 497, 497, 497, 497, 497, 497, 497, 497, 497,\n",
      "       497, 497, 497, 497, 497, 497, 497, 497, 497, 497, 497, 497, 497,\n",
      "       497, 497, 497, 497, 497, 497, 497, 497, 497, 497, 497, 497, 497,\n",
      "       497, 497, 497, 497, 497, 497, 497, 497, 497, 497, 497, 497, 497,\n",
      "       497, 497, 497, 497, 497, 497, 497, 497, 497, 497, 497, 497, 497,\n",
      "       497, 497, 497, 497, 497, 497, 497, 497, 498, 498, 498, 498, 498,\n",
      "       498, 498, 498, 498, 498, 498, 498, 498, 498, 498, 702, 702, 702,\n",
      "       702, 702, 702, 711, 711, 711, 711, 711, 711, 711, 711, 711, 711,\n",
      "       711, 711, 711, 711, 711, 711, 712, 712, 712, 752, 752, 752, 752,\n",
      "       752], dtype=int64), array([  0,   0,   0,   1,   1,   1,   2,   2, 129,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   2,\n",
      "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "         2,   2,   2,   2,   2,   2,   2,   3,   3,   3,   3,   3,   3,\n",
      "         3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   3,   4,   4,\n",
      "         4,   4,   4,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         1,   1,   1,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "         2,   2,   2,   2,   3,   3,   3,   3,   3,   3,   3,   3,   3,\n",
      "         3,   3,   3,   3,   3,   3,   4,   4,   4,   4,   4,   4,   4,\n",
      "         4,   4,   4,   4,   4,   4,   4,   5,   5,   5,   5,   5,   5,\n",
      "         5,   5,   5,   5,   5,   5,   5,   5,   6,   6,   6,   6,   6,\n",
      "         6,   6,   6,   6,   6,   6,   7,   7,   7,   7,   7,   7,   7,\n",
      "         7,   7,   7,   8,   8,   8,   8,   8,   8,   8,   9,   1,   2,\n",
      "         2,   3,   3,   3,   3,   3,   3,   3,   3,   4,   4,   4,   4,\n",
      "         4,   4,   4,   4,   4,   5,   5,   5,   5,   5,   5,   5,   5,\n",
      "         5,   6,   6,   6,   6,   6,   6,   6,   6,   7,   7,   7,   7,\n",
      "         7,   7,   7,   7,   8,   8,   8,   8,   8,   8,   8,   9,   9,\n",
      "         9,   9,   9,   9,  70,  71,  71,  62,  63,  49,  50,  51,   9,\n",
      "        10,  10,  14,  35,  55,  58,  59,  60,  44,  45,  45,  48,  48,\n",
      "        48,  48,  48,  49,  49,  49,  49,  49,  49,  50,  50,  50,  50,\n",
      "        50,  50,  50,  50,  51,  51,  51,  51,  51,  51,  52,  52,  52,\n",
      "        52,  71,  72,  72,  73,  73,  51,  51,  52,  52,  52,  52,  52,\n",
      "        52,  52,  53,  53,  53,  53,  53,  53,  53,  53,  53,  54,  54,\n",
      "        54,  54,  54,  54,  54,  55,  55,  55,  55,  40,  40,  40,  41,\n",
      "        41,  41,  41,  41,  41,  42,  42,  42,  42,  42,  42,  43,  43,\n",
      "        43,  35,  36,  36,  12,   1,   1,   2,   2,   2,   2,   3,   3,\n",
      "         3,   3,   4,   4,   4,   4,   5,   5,   5,   5,   5,   6,   6,\n",
      "         6,   6,   6,   6,   7,   7,   7,   7,   7,   8,   8,   8,   9,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   1,\n",
      "         1,   1,   1,   2,   2,   2,   2,   2,   2,   2,   3,   3,   3,\n",
      "         3,   3,   3,   4,   4,   4,   4,   4,   5,   5,   5,   5,   6,\n",
      "         6,   6,   7,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,\n",
      "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "         2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,   2,\n",
      "         3,   3,   3,   3,   3,   3,   3,   3,   4,   4,   4,   4,   4,\n",
      "         5,   5,   5,   6,   1,   1,   1,   2,   2,   2,   2,   3,   3,\n",
      "         3,   3,   3,   3,   3,   3,   3,   3,   4,   4,   4,   4,   4,\n",
      "         4,   4,   4,  25,  25,  26,  20,  21,  21,  22,  22,  22,  23,\n",
      "        21,  22,  23,  21,  23,  23,  23,  24,  24,  24,  24,  24,  24,\n",
      "        25,  25,  25,  25,  25,  25,  26,  26,  26,  26,  26,  26,  27,\n",
      "        27,  27,  27,  27,  27,  27,  28,  28,  28,  28,  28,  28,  28,\n",
      "        28,  29,  29,  29,  29,  29,  30,  30,  19,  19,  19,  20,  21,\n",
      "        21,  21,  21,  22,  22,  22,  22,  22,  22,  22,  22,  22,  22,\n",
      "        22,  23,  23,  23,  23,  23,  23,  23,  23,  23,  23,  23,  24,\n",
      "        24,  24,  24,  24,  24,  24,  24,  24,  24,  24,  24,  25,  25,\n",
      "        25,  25,  25,  25,  25,  25,  25,  25,  25,  25,  25,  26,  26,\n",
      "        26,  26,  26,  26,  26,  26,  26,  26,  26,  26,  26,  27,  27,\n",
      "        27,  27,  27,  27,  27,  27,  27,  27,  27,  27,  27,  28,  28,\n",
      "        28,  28,  28,  28,  28,  28,  28,  28,  29,  29,  29,  29,  29,\n",
      "        29,  29,  30,  30,  30,  30,  30,  30,  24,  25,  25,  25,  25,\n",
      "        26,  26,  26,  26,  27,  27,  27,  28,  28,  28,  25,  26,  26,\n",
      "        26,  27,  27,  49,  49,  49,  49,  50,  50,  50,  51,  51,  51,\n",
      "        52,  52,  52,  53,  53,  54,  56,  56,  57,  74,  75,  75,  76,\n",
      "        77], dtype=int64), array([  0,   1,   2,   0,   1,   2,   0,   1, 179,   0,   1,   2,   3,\n",
      "         9,  10,  11,  12,  15,  16,  17,  18,  19,  20,  21,  22,  23,\n",
      "        24,  25,  26,  27,  28,   0,   1,   2,   3,   8,   9,  10,  11,\n",
      "        12,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,   0,\n",
      "         1,   2,   7,   8,   9,  10,  11,  12,  15,  16,  17,  18,  19,\n",
      "        20,  21,  22,  23,  24,  25,  26,   0,   1,   2,   8,   9,  10,\n",
      "        11,  12,  16,  17,  18,  19,  20,  21,  22,  23,  24,  19,  20,\n",
      "        21,  22,  23,   2,   3,   4,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,   1,   2,   3,   4,\n",
      "         5,   6,   7,   8,   9,  10,  11,  12,  13,  14,  15,  16,  17,\n",
      "        18,  19,  20,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "        14,  15,  16,  17,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n",
      "        10,  11,  12,  13,  14,  15,   1,   2,   3,   4,   5,   6,   7,\n",
      "         8,   9,  10,  11,  12,  13,  14,   1,   2,   3,   4,   5,   6,\n",
      "         7,   8,   9,  10,  11,  12,  13,  14,   4,   5,   6,   7,   8,\n",
      "         9,  10,  11,  12,  13,  14,   5,   6,   7,   8,   9,  10,  11,\n",
      "        12,  13,  14,   7,   8,   9,  10,  11,  12,  13,  10,   0,   0,\n",
      "         1,   0,   1,   2,   3,   4,   6,   7,   8,   0,   1,   2,   3,\n",
      "         4,   5,   6,   7,   8,   0,   1,   2,   3,   4,   5,   6,   7,\n",
      "         8,   0,   1,   2,   3,   4,   5,   6,   7,   0,   1,   2,   3,\n",
      "         4,   5,   6,   7,   0,   1,   2,   3,   4,   5,   6,   0,   1,\n",
      "         2,   3,   4,   5,  21,  21,  22,  31,  31,  50,  50,  50,   5,\n",
      "         4,   5,   0,  87,   0,   0,   0,   0,  18,  18,  19,  13,  14,\n",
      "        15,  16,  17,  13,  14,  15,  16,  17,  18,  11,  12,  13,  14,\n",
      "        15,  16,  17,  18,  12,  13,  14,  15,  16,  17,  13,  14,  15,\n",
      "        16,   0,   1,   2,   1,   2,  10,  24,   8,   9,  10,  11,  12,\n",
      "        13,  24,   6,   7,   8,   9,  10,  11,  12,  13,  14,   6,   7,\n",
      "         8,   9,  10,  11,  12,   7,   8,   9,  10,  17,  18,  19,  14,\n",
      "        15,  16,  17,  18,  19,  12,  13,  14,  15,  16,  17,  12,  13,\n",
      "        14,   8,   7,   8,   5,   0,   1,   0,  86,  87,  88,  84,  85,\n",
      "        86,  87,  83,  84,  85,  86,  82,  83,  84,  85,  86,  81,  82,\n",
      "        83,  84,  85,  86,  81,  82,  83,  84,  85,  82,  83,  84,  83,\n",
      "         0,   0,   1,   2,   3,   4,   5,   6,   7,   0,   1,   2,   3,\n",
      "         4,   5,   6,   0,   1,   2,   3,   4,   5,   6,   0,   1,   2,\n",
      "         3,   4,   5,   0,   1,   2,   3,   4,   0,   1,   2,   3,   0,\n",
      "         1,   2,   0,   0,   1,   2,   3,   4,   5,   6,   7,   8,  13,\n",
      "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,   0,   1,   2,\n",
      "         3,   4,   5,   6,   7,  13,  14,  15,  16,  17,  18,  19,  20,\n",
      "         0,   1,   2,   3,   4,   5,   6,   7,  16,  17,  18,  19,  20,\n",
      "         0,   1,   2,   3,   4,   5,   6,  17,   0,   1,   2,   3,   4,\n",
      "         0,   1,   2,   0,  13,  14,  15,  14,  15,  16,  17,   9,  10,\n",
      "        11,  12,  13,  14,  15,  16,  17,  18,  10,  11,  12,  13,  14,\n",
      "        15,  16,  17,  27,  28,  27,  30,  28,  29,  27,  28,  29,  28,\n",
      "        28,  28,  28,  28,  29,  30,  31,  27,  28,  29,  30,  31,  32,\n",
      "        27,  28,  29,  30,  31,  32,  27,  28,  29,  30,  31,  32,  26,\n",
      "        27,  28,  29,  30,  31,  32,  25,  26,  27,  28,  29,  30,  31,\n",
      "        32,  27,  28,  29,  30,  31,  28,  29,  38,  39,  40,  37,  28,\n",
      "        36,  37,  38,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,\n",
      "        38,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  27,\n",
      "        28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  27,  28,\n",
      "        29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  27,  28,\n",
      "        29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  27,  28,\n",
      "        29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  25,  26,\n",
      "        27,  28,  33,  34,  35,  36,  37,  38,  25,  26,  27,  34,  35,\n",
      "        36,  37,  25,  26,  27,  28,  34,  35,  39,  36,  38,  39,  40,\n",
      "        37,  38,  39,  40,  37,  38,  39,  37,  38,  39,  46,  47,  48,\n",
      "        49,  48,  49,  72,  73,  74,  75,  74,  75,  76,  73,  74,  75,\n",
      "        72,  73,  74,  72,  73,  72,  72,  73,  74,   3,   2,   3,   2,\n",
      "         1], dtype=int64))\n",
      "2024-11-17 21:13:48,907 - SubseasonalPredictor - INFO - 准备数据...\n",
      "2024-11-17 21:13:48,943 - SubseasonalPredictor - ERROR - 程序执行出错: dimension valid_time on 0th function argument to apply_ufunc with dask='parallelized' consists of multiple chunks, but is also a core dimension. To fix, either rechunk into a single array chunk along this dimension, i.e., ``.chunk(dict(valid_time=-1))``, or pass ``allow_rechunk=True`` in ``dask_gufunc_kwargs`` but beware that this may significantly increase memory usage.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dimension valid_time on 0th function argument to apply_ufunc with dask='parallelized' consists of multiple chunks, but is also a core dimension. To fix, either rechunk into a single array chunk along this dimension, i.e., ``.chunk(dict(valid_time=-1))``, or pass ``allow_rechunk=True`` in ``dask_gufunc_kwargs`` but beware that this may significantly increase memory usage.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 259\u001b[0m\n\u001b[0;32m    256\u001b[0m         gc\u001b[39m.\u001b[39mcollect()\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 259\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[12], line 230\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    228\u001b[0m predictor\u001b[39m.\u001b[39mvalidate_data_continuity(ds)\n\u001b[0;32m    229\u001b[0m predictor\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39m准备数据...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 230\u001b[0m X, y \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39;49mprepare_data(ds)\n\u001b[0;32m    231\u001b[0m \u001b[39m# 划分训练集和测试集\u001b[39;00m\n\u001b[0;32m    232\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(\n\u001b[0;32m    233\u001b[0m     X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m\n\u001b[0;32m    234\u001b[0m )\n",
      "Cell \u001b[1;32mIn[12], line 77\u001b[0m, in \u001b[0;36mSubseasonalPredictor.prepare_data\u001b[1;34m(self, ds, target_lead_time)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m缺少必需的变量: \u001b[39m\u001b[39m{\u001b[39;00mmissing_vars\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[39m# 处理缺失值\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m ds \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49minterpolate_na(dim\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mvalid_time\u001b[39;49m\u001b[39m'\u001b[39;49m, method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlinear\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     78\u001b[0m ds \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39mffill(\u001b[39m'\u001b[39m\u001b[39mvalid_time\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mbfill(\u001b[39m'\u001b[39m\u001b[39mvalid_time\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     79\u001b[0m ds \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39msortby(\u001b[39m'\u001b[39m\u001b[39mvalid_time\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\environment-config\\lib\\site-packages\\xarray\\core\\dataset.py:6718\u001b[0m, in \u001b[0;36mDataset.interpolate_na\u001b[1;34m(self, dim, method, limit, use_coordinate, max_gap, **kwargs)\u001b[0m\n\u001b[0;32m   6600\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fill in NaNs by interpolating according to different methods.\u001b[39;00m\n\u001b[0;32m   6601\u001b[0m \n\u001b[0;32m   6602\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6714\u001b[0m \u001b[39m    D        (x) float64 40B 5.0 3.0 1.0 -1.0 4.0\u001b[39;00m\n\u001b[0;32m   6715\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   6716\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mxarray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmissing\u001b[39;00m \u001b[39mimport\u001b[39;00m _apply_over_vars_with_dim, interp_na\n\u001b[1;32m-> 6718\u001b[0m new \u001b[39m=\u001b[39m _apply_over_vars_with_dim(\n\u001b[0;32m   6719\u001b[0m     interp_na,\n\u001b[0;32m   6720\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   6721\u001b[0m     dim\u001b[39m=\u001b[39mdim,\n\u001b[0;32m   6722\u001b[0m     method\u001b[39m=\u001b[39mmethod,\n\u001b[0;32m   6723\u001b[0m     limit\u001b[39m=\u001b[39mlimit,\n\u001b[0;32m   6724\u001b[0m     use_coordinate\u001b[39m=\u001b[39muse_coordinate,\n\u001b[0;32m   6725\u001b[0m     max_gap\u001b[39m=\u001b[39mmax_gap,\n\u001b[0;32m   6726\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   6727\u001b[0m )\n\u001b[0;32m   6728\u001b[0m \u001b[39mreturn\u001b[39;00m new\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\environment-config\\lib\\site-packages\\xarray\\core\\missing.py:220\u001b[0m, in \u001b[0;36m_apply_over_vars_with_dim\u001b[1;34m(func, self, dim, **kwargs)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[39mfor\u001b[39;00m name, var \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_vars\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    219\u001b[0m     \u001b[39mif\u001b[39;00m dim \u001b[39min\u001b[39;00m var\u001b[39m.\u001b[39mdims:\n\u001b[1;32m--> 220\u001b[0m         ds[name] \u001b[39m=\u001b[39m func(var, dim\u001b[39m=\u001b[39mdim, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    221\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m         ds[name] \u001b[39m=\u001b[39m var\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\environment-config\\lib\\site-packages\\xarray\\core\\missing.py:365\u001b[0m, in \u001b[0;36minterp_na\u001b[1;34m(self, dim, use_coordinate, method, limit, max_gap, keep_attrs, **kwargs)\u001b[0m\n\u001b[0;32m    363\u001b[0m     warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39moverflow\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mRuntimeWarning\u001b[39;00m)\n\u001b[0;32m    364\u001b[0m     warnings\u001b[39m.\u001b[39mfilterwarnings(\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39minvalid value\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mRuntimeWarning\u001b[39;00m)\n\u001b[1;32m--> 365\u001b[0m     arr \u001b[39m=\u001b[39m apply_ufunc(\n\u001b[0;32m    366\u001b[0m         interpolator,\n\u001b[0;32m    367\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m    368\u001b[0m         index,\n\u001b[0;32m    369\u001b[0m         input_core_dims\u001b[39m=\u001b[39;49m[[dim], [dim]],\n\u001b[0;32m    370\u001b[0m         output_core_dims\u001b[39m=\u001b[39;49m[[dim]],\n\u001b[0;32m    371\u001b[0m         output_dtypes\u001b[39m=\u001b[39;49m[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype],\n\u001b[0;32m    372\u001b[0m         dask\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mparallelized\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    373\u001b[0m         vectorize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    374\u001b[0m         keep_attrs\u001b[39m=\u001b[39;49mkeep_attrs,\n\u001b[0;32m    375\u001b[0m     )\u001b[39m.\u001b[39mtranspose(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdims)\n\u001b[0;32m    377\u001b[0m \u001b[39mif\u001b[39;00m limit \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m     arr \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mwhere(valids)\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\environment-config\\lib\\site-packages\\xarray\\core\\computation.py:1268\u001b[0m, in \u001b[0;36mapply_ufunc\u001b[1;34m(func, input_core_dims, output_core_dims, exclude_dims, vectorize, join, dataset_join, dataset_fill_value, keep_attrs, kwargs, dask, output_dtypes, output_sizes, meta, dask_gufunc_kwargs, on_missing_core_dim, *args)\u001b[0m\n\u001b[0;32m   1266\u001b[0m \u001b[39m# feed DataArray apply_variable_ufunc through apply_dataarray_vfunc\u001b[39;00m\n\u001b[0;32m   1267\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(a, DataArray) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args):\n\u001b[1;32m-> 1268\u001b[0m     \u001b[39mreturn\u001b[39;00m apply_dataarray_vfunc(\n\u001b[0;32m   1269\u001b[0m         variables_vfunc,\n\u001b[0;32m   1270\u001b[0m         \u001b[39m*\u001b[39;49margs,\n\u001b[0;32m   1271\u001b[0m         signature\u001b[39m=\u001b[39;49msignature,\n\u001b[0;32m   1272\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[0;32m   1273\u001b[0m         exclude_dims\u001b[39m=\u001b[39;49mexclude_dims,\n\u001b[0;32m   1274\u001b[0m         keep_attrs\u001b[39m=\u001b[39;49mkeep_attrs,\n\u001b[0;32m   1275\u001b[0m     )\n\u001b[0;32m   1276\u001b[0m \u001b[39m# feed Variables directly through apply_variable_ufunc\u001b[39;00m\n\u001b[0;32m   1277\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(a, Variable) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args):\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\environment-config\\lib\\site-packages\\xarray\\core\\computation.py:312\u001b[0m, in \u001b[0;36mapply_dataarray_vfunc\u001b[1;34m(func, signature, join, exclude_dims, keep_attrs, *args)\u001b[0m\n\u001b[0;32m    307\u001b[0m result_coords, result_indexes \u001b[39m=\u001b[39m build_output_coords_and_indexes(\n\u001b[0;32m    308\u001b[0m     args, signature, exclude_dims, combine_attrs\u001b[39m=\u001b[39mkeep_attrs\n\u001b[0;32m    309\u001b[0m )\n\u001b[0;32m    311\u001b[0m data_vars \u001b[39m=\u001b[39m [\u001b[39mgetattr\u001b[39m(a, \u001b[39m\"\u001b[39m\u001b[39mvariable\u001b[39m\u001b[39m\"\u001b[39m, a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args]\n\u001b[1;32m--> 312\u001b[0m result_var \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49mdata_vars)\n\u001b[0;32m    314\u001b[0m out: \u001b[39mtuple\u001b[39m[DataArray, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m] \u001b[39m|\u001b[39m DataArray\n\u001b[0;32m    315\u001b[0m \u001b[39mif\u001b[39;00m signature\u001b[39m.\u001b[39mnum_outputs \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32me:\\Anaconda3\\envs\\environment-config\\lib\\site-packages\\xarray\\core\\computation.py:767\u001b[0m, in \u001b[0;36mapply_variable_ufunc\u001b[1;34m(func, signature, exclude_dims, dask, output_dtypes, vectorize, keep_attrs, dask_gufunc_kwargs, *args)\u001b[0m\n\u001b[0;32m    765\u001b[0m             \u001b[39mfor\u001b[39;00m axis, dim \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(core_dims, start\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(core_dims)):\n\u001b[0;32m    766\u001b[0m                 \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mchunks[axis]) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 767\u001b[0m                     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    768\u001b[0m                         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdimension \u001b[39m\u001b[39m{\u001b[39;00mdim\u001b[39m}\u001b[39;00m\u001b[39m on \u001b[39m\u001b[39m{\u001b[39;00mn\u001b[39m}\u001b[39;00m\u001b[39mth function argument to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    769\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mapply_ufunc with dask=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mparallelized\u001b[39m\u001b[39m'\u001b[39m\u001b[39m consists of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    770\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mmultiple chunks, but is also a core dimension. To \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    771\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mfix, either rechunk into a single array chunk along \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    772\u001b[0m                         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthis dimension, i.e., ``.chunk(dict(\u001b[39m\u001b[39m{\u001b[39;00mdim\u001b[39m}\u001b[39;00m\u001b[39m=-1))``, or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    773\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mpass ``allow_rechunk=True`` in ``dask_gufunc_kwargs`` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    774\u001b[0m                         \u001b[39m\"\u001b[39m\u001b[39mbut beware that this may significantly increase memory usage.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    775\u001b[0m                     )\n\u001b[0;32m    776\u001b[0m     dask_gufunc_kwargs[\u001b[39m\"\u001b[39m\u001b[39mallow_rechunk\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    778\u001b[0m output_sizes \u001b[39m=\u001b[39m dask_gufunc_kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39moutput_sizes\u001b[39m\u001b[39m\"\u001b[39m, {})\n",
      "\u001b[1;31mValueError\u001b[0m: dimension valid_time on 0th function argument to apply_ufunc with dask='parallelized' consists of multiple chunks, but is also a core dimension. To fix, either rechunk into a single array chunk along this dimension, i.e., ``.chunk(dict(valid_time=-1))``, or pass ``allow_rechunk=True`` in ``dask_gufunc_kwargs`` but beware that this may significantly increase memory usage."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import dask\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # 忽略警告信息\n",
    "\n",
    "class SubseasonalPredictor:\n",
    "    def __init__(self, base_dir='data'):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = None\n",
    "        self.logger = self._setup_logger()\n",
    "\n",
    "    def _setup_logger(self):\n",
    "        \"\"\"设置日志记录器\"\"\"\n",
    "        logger = logging.getLogger('SubseasonalPredictor')\n",
    "        logger.setLevel(logging.INFO)\n",
    "        # 避免重复添加处理器\n",
    "        if not logger.handlers:\n",
    "            ch = logging.StreamHandler()\n",
    "            ch.setLevel(logging.INFO)\n",
    "            fh = logging.FileHandler('subseasonal_prediction.log')\n",
    "            fh.setLevel(logging.INFO)\n",
    "            formatter = logging.Formatter(\n",
    "                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "            )\n",
    "            ch.setFormatter(formatter)\n",
    "            fh.setFormatter(formatter)\n",
    "            logger.addHandler(ch)\n",
    "            logger.addHandler(fh)\n",
    "        return logger\n",
    "\n",
    "    def validate_data_continuity(self, ds):\n",
    "        \"\"\"验证数据的时间连续性和质量\"\"\"\n",
    "        # 检查时间步长\n",
    "        time_diff = np.diff(ds.valid_time.values)\n",
    "        expected_diff = np.timedelta64(6, 'h')  # 6小时间隔\n",
    "        if not np.all(time_diff == expected_diff):\n",
    "            problematic_times = np.where(time_diff != expected_diff)[0]\n",
    "            self.logger.warning(f\"发现时间不连续: {problematic_times}\")\n",
    "        # 检查异常值\n",
    "        for var in ['tp', 't2m', 'msl', 'u10', 'v10']:\n",
    "            if var in ds.data_vars:\n",
    "                data = ds[var].values\n",
    "                if np.issubdtype(data.dtype, np.number):\n",
    "                    mean = np.nanmean(data)\n",
    "                    std = np.nanstd(data)\n",
    "                    outliers = np.where(np.abs(data - mean) > 5 * std)\n",
    "                    if len(outliers[0]) > 0:\n",
    "                        self.logger.warning(f\"{var}存在异常值，位置: {outliers}\")\n",
    "                else:\n",
    "                    self.logger.info(f\"{var}不是数值类型，跳过异常值检查\")\n",
    "        # 检查缺失值比例\n",
    "        for var in ds.data_vars:\n",
    "            missing_ratio = ds[var].isnull().mean().values\n",
    "            if missing_ratio > 0:\n",
    "                self.logger.warning(\n",
    "                    f\"{var}存在缺失值，比例: {missing_ratio:.2%}\"\n",
    "                )\n",
    "\n",
    "    def prepare_data(self, ds, target_lead_time=28):\n",
    "        \"\"\"准备神经网络输入数据\"\"\"\n",
    "        required_vars = ['tp', 't2m', 'msl', 'u10', 'v10']\n",
    "        missing_vars = [var for var in required_vars\n",
    "                        if var not in ds.data_vars]\n",
    "        if missing_vars:\n",
    "            raise ValueError(f\"缺少必需的变量: {missing_vars}\")\n",
    "        # 处理缺失值\n",
    "        ds = ds.interpolate_na(dim='valid_time', method='linear')\n",
    "        ds = ds.ffill('valid_time').bfill('valid_time')\n",
    "        ds = ds.sortby('valid_time')\n",
    "        # 处理降水累积值\n",
    "        if 'tp' in ds.data_vars:\n",
    "            ds['tp'] = ds['tp'].diff('valid_time').fillna(0)\n",
    "            ds['tp'] = xr.where(ds['tp'] < 0, 0, ds['tp'])\n",
    "        # 标准化数据\n",
    "        for var in required_vars:\n",
    "            if var in ds.data_vars:\n",
    "                mean = ds[var].mean()\n",
    "                std = ds[var].std()\n",
    "                ds[var] = (ds[var] - mean) / std\n",
    "        # 提取热带区域降水数据\n",
    "        tropical_precip = ds['tp'].sel(latitude=slice(26, -26))\n",
    "        # 定义欧洲三个区域\n",
    "        regions = {\n",
    "            'north_europe': {'latitude': slice(65, 55),\n",
    "                             'longitude': slice(-10, 30)},\n",
    "            'central_europe': {'latitude': slice(55, 45),\n",
    "                               'longitude': slice(-5, 20)},\n",
    "            'south_europe': {'latitude': slice(45, 35),\n",
    "                             'longitude': slice(-10, 30)}\n",
    "        }\n",
    "        targets = {}\n",
    "        for region, coords in regions.items():\n",
    "            regional_precip = ds['tp'].sel(**coords)\n",
    "            targets[region] = regional_precip.mean(\n",
    "                dim=['latitude', 'longitude']\n",
    "            )\n",
    "        # 创建输入和输出数据\n",
    "        X_list, y_list = [], []\n",
    "        for i in range(len(ds.valid_time) - target_lead_time):\n",
    "            X_sample = tropical_precip.isel(\n",
    "                valid_time=i\n",
    "            ).values\n",
    "            y_sample = []\n",
    "            for region in regions:\n",
    "                y_value = targets[region].isel(\n",
    "                    valid_time=i + target_lead_time\n",
    "                ).values\n",
    "                y_sample.append(np.sign(y_value))\n",
    "            X_list.append(X_sample)\n",
    "            y_list.append(y_sample)\n",
    "        X = np.array(X_list)\n",
    "        y = np.array(y_list)\n",
    "        return X, y\n",
    "\n",
    "    def build_model(self, input_shape):\n",
    "        \"\"\"构建神经网络模型\"\"\"\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(\n",
    "                32, (3, 3), activation='relu', input_shape=input_shape\n",
    "            ),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(3, activation='softmax')\n",
    "        ])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val,\n",
    "              epochs=50, batch_size=32):\n",
    "        \"\"\"训练模型\"\"\"\n",
    "        input_shape = X_train.shape[1:]\n",
    "        self.model = self.build_model(input_shape)\n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=5,\n",
    "                    restore_best_weights=True\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        return history\n",
    "\n",
    "    def explain_predictions(self, X_test, background_samples=100):\n",
    "        \"\"\"使用SHAP解释模型预测\"\"\"\n",
    "        explainer = shap.DeepExplainer(\n",
    "            self.model, X_test[:background_samples]\n",
    "        )\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        return shap_values\n",
    "\n",
    "    def plot_feature_importance(self, shap_values, region_idx=0):\n",
    "        \"\"\"绘制特征重要性图\"\"\"\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        shap.summary_plot(\n",
    "            shap_values[region_idx],\n",
    "            plot_type='bar',\n",
    "            feature_names=[\n",
    "                f'Feature_{i}' for i in range(shap_values[0].shape[1])\n",
    "            ]\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "    def analyze_decadal_skill(self, predictions, true_values,\n",
    "                              window_size=3650):\n",
    "        \"\"\"分析预测技巧的年代际变化\"\"\"\n",
    "        accuracies = []\n",
    "        dates = []\n",
    "        for i in range(0, len(predictions) - window_size, 365):\n",
    "            window_preds = predictions[i:i + window_size]\n",
    "            window_true = true_values[i:i + window_size]\n",
    "            accuracy = np.mean(window_preds == window_true)\n",
    "            accuracies.append(accuracy)\n",
    "            dates.append(i / 365 + 1959)  # 假设从1959年开始\n",
    "        return dates, accuracies\n",
    "\n",
    "    def plot_decadal_skill(self, dates, accuracies, region_name):\n",
    "        \"\"\"绘制预测技巧的年代际变化\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(dates, accuracies)\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('10-year Moving Average Accuracy')\n",
    "        plt.title(f'Decadal Variation in Prediction Skill: {region_name}')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数，包含内存管理和错误处理\"\"\"\n",
    "    # 设置dask配置\n",
    "    dask.config.set(\n",
    "        {\"array.slicing.split_large_chunks\": True,\n",
    "         \"array.chunk-size\": \"100MB\"}\n",
    "    )\n",
    "    predictor = SubseasonalPredictor()\n",
    "    try:\n",
    "        predictor.logger.info(\"加载数据...\")\n",
    "        ds = xr.open_mfdataset(\n",
    "            'era5_data/era5_*.nc',\n",
    "            chunks={'valid_time': -1,\n",
    "                    'latitude': 'auto',\n",
    "                    'longitude': 'auto'},\n",
    "            combine='by_coords'\n",
    "        )\n",
    "        predictor.logger.info(\"验证数据质量...\")\n",
    "        predictor.validate_data_continuity(ds)\n",
    "        predictor.logger.info(\"准备数据...\")\n",
    "        X, y = predictor.prepare_data(ds)\n",
    "        # 划分训练集和测试集\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        predictor.logger.info(\"训练模型...\")\n",
    "        history = predictor.train(X_train, y_train, X_test, y_test)\n",
    "        predictor.logger.info(\"模型解释...\")\n",
    "        shap_values = predictor.explain_predictions(X_test)\n",
    "        predictor.plot_feature_importance(shap_values)\n",
    "        predictor.logger.info(\"分析预测技巧...\")\n",
    "        predictions = predictor.model.predict(X_test)\n",
    "        for i, region in enumerate(['North', 'Central', 'South']):\n",
    "            pred_labels = np.argmax(predictions, axis=1)\n",
    "            true_labels = np.argmax(y_test, axis=1)\n",
    "            dates, accuracies = predictor.analyze_decadal_skill(\n",
    "                pred_labels, true_labels\n",
    "            )\n",
    "            predictor.plot_decadal_skill(dates, accuracies, region)\n",
    "            plt.savefig(f'decadal_skill_{region.lower()}.png')\n",
    "            plt.close()\n",
    "    except Exception as e:\n",
    "        predictor.logger.error(f\"程序执行出错: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "14b1b810ea4ed5e945a5a903fe851e2c1cad618f6da58d66591ada60e62a8ab4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
